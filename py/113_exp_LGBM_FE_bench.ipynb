{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is the purpose of this experiment?\n",
    "LGBMのkernelにおける学習時間を計測する\n",
    "### 2. Why do you this?\n",
    "CPUで攻めるかGPUで攻めるか判断する.GPUの場合もLGBMを使うか判断する\n",
    "### 3. Where are the points of technology and techniques?\n",
    "LGBM\n",
    "### 4. How do you validate the effectiveness?\n",
    "処理時間\n",
    "### 5. What will you do next?\n",
    "NN * FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T09:53:43.606080Z",
     "start_time": "2018-11-10T09:53:43.430947Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-26 08:56:37,404 __main__ 51 [INFO]    [logger_func] start \n"
     ]
    }
   ],
   "source": [
    "is_local = False\n",
    "seed = 1208\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import datetime\n",
    "import sys\n",
    "import re\n",
    "import gc\n",
    "import glob\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "def pararell_process(func, arg_list, cpu_cnt=multiprocessing.cpu_count()):\n",
    "    process = Pool(cpu_cnt)\n",
    "    callback = process.map_async(func, arg_list).get(600)\n",
    "    process.close()\n",
    "    process.terminate()\n",
    "    return callback\n",
    "\n",
    "def mkdir_func(path):\n",
    "    try:\n",
    "        os.stat(path)\n",
    "    except:\n",
    "        os.mkdir(path)\n",
    "\n",
    "from logging import StreamHandler, DEBUG, Formatter, FileHandler, getLogger\n",
    "def logger_func():\n",
    "    logger = getLogger(__name__)\n",
    "    log_fmt = Formatter('%(asctime)s %(name)s %(lineno)d [%(levelname)s]\\\n",
    "    [%(funcName)s] %(message)s ')\n",
    "    handler = StreamHandler()\n",
    "    handler.setLevel('INFO')\n",
    "    handler.setFormatter(log_fmt)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    mkdir_func('../output')\n",
    "    handler = FileHandler('../output/py_train.py.log', 'a')\n",
    "    handler.setLevel(DEBUG)\n",
    "    handler.setFormatter(log_fmt)\n",
    "    logger.setLevel(DEBUG)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    logger.info('start')\n",
    "\n",
    "    return logger\n",
    "logger = logger_func()\n",
    "\n",
    "st_time = time.time()\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    \"\"\"\n",
    "    Taken from Konstantin Lopuhin https://www.kaggle.com/lopuhin\n",
    "    in script named : Mercari Golf: 0.3875 CV in 75 LOC, 1900 s\n",
    "    https://www.kaggle.com/lopuhin/mercari-golf-0-3875-cv-in-75-loc-1900-s\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "    \n",
    "# NLP\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import string\n",
    "\n",
    "key = 'qid'\n",
    "qt = 'question_text'\n",
    "target = 'target'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contraction replacement patterns\n",
    "cont_patterns = [\n",
    "    (b'(W|w)on\\'t', b'will not'),\n",
    "    (b'(C|c)an\\'t', b'can not'),\n",
    "    (b'(I|i)\\'m', b'i am'),\n",
    "    (b'(A|a)in\\'t', b'is not'),\n",
    "    (b'(\\w+)\\'ll', b'\\g<1> will'),\n",
    "    (b'(\\w+)n\\'t', b'\\g<1> not'),\n",
    "    (b'(\\w+)\\'ve', b'\\g<1> have'),\n",
    "    (b'(\\w+)\\'s', b'\\g<1> is'),\n",
    "    (b'(\\w+)\\'re', b'\\g<1> are'),\n",
    "    (b'(\\w+)\\'d', b'\\g<1> would'),\n",
    "]\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "\n",
    "def prepare_for_char_n_gram(text):\n",
    "    \"\"\" Simple text clean up process\"\"\"\n",
    "    # 1. Go to lower case (only good for english)\n",
    "    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n",
    "    clean = bytes(text.lower(), encoding=\"utf-8\")\n",
    "    # 2. Drop \\n and  \\t\n",
    "    clean = clean.replace(b\"\\n\", b\" \")\n",
    "    clean = clean.replace(b\"\\t\", b\" \")\n",
    "    clean = clean.replace(b\"\\b\", b\" \")\n",
    "    clean = clean.replace(b\"\\r\", b\" \")\n",
    "    # 3. Replace english contractions\n",
    "    for (pattern, repl) in patterns:\n",
    "        clean = re.sub(pattern, repl, clean)\n",
    "    # 4. Drop puntuation\n",
    "    # I could have used regex package with regex.sub(b\"\\p{P}\", \" \")\n",
    "    exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n",
    "    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n",
    "    # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n",
    "    clean = re.sub(b\"\\d+\", b\" \", clean)\n",
    "    # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n",
    "    clean = re.sub(b'\\s+', b' ', clean)\n",
    "    # Remove ending space if any\n",
    "    clean = re.sub(b'\\s+$', b'', clean)\n",
    "    # 7. Now replace words by words surrounded by # signs\n",
    "    # e.g. my name is bond would become #my# #name# #is# #bond#\n",
    "    # clean = re.sub(b\"([a-z]+)\", b\"#\\g<1>#\", clean)\n",
    "#     clean = re.sub(b\" \", b\"# #\", clean)  # Replace space\n",
    "#     clean = b\"#\" + clean + b\"#\"  # add leading and trailing #\n",
    "\n",
    "    return str(clean, 'utf-8')\n",
    "\n",
    "def count_regexp_occ(regexp=\"\", text=None):\n",
    "    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n",
    "    return len(re.findall(regexp, text))\n",
    "\n",
    "def cleansing_text(text, remove_stopwords=True):\n",
    "\n",
    "    # Convert words to lower case and split them\n",
    "    text = re.sub(\"_\", \" \", text, flags=re.IGNORECASE)\n",
    "    text = text.lower().split()\n",
    "    regex_num = re.compile(u\"[0-9０-９]\")\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = STOPWORDS\n",
    "        text = [w for w in text if (not w in stops) and not(regex_num.match(w))]\n",
    "\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"[c-fC-F]\\:\\/\", \" disk \", text)\n",
    "    text = re.sub(\"\\'d\", \" would \", text)\n",
    "    text = re.sub(\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(\"\\'re\", \" are \", text)\n",
    "    text = re.sub(\"\\'s\", \" \", text) # we have cases like \"Sam is\" or \"Sam's\" (i.e. his) these two cases aren't separable, I choose to compromise are kill \"'s\" directly\n",
    "    text = re.sub(\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(\"\\(s\\)\", \" \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"`\", \"'\", text) # special single quote\n",
    "    text = re.sub(\"e-mail\", \" email \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(\"b\\.g\\.\", \" bg \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"e\\.g\\.\", \" eg \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"i'm\", \"i am\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"é\", \"e\", text)\n",
    "    text = re.sub(\"’\", \"'\", text) # special single quote\n",
    "    text = re.sub(\"“\", '\"', text) # special double quote\n",
    "    text = re.sub(\"…\", \" \", text)\n",
    "    text = re.sub(\"？\", \"?\", text)\n",
    "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
    "    text = re.sub('\\$', \" dollar \", text)\n",
    "    text = re.sub('\\%', \" percent \", text)\n",
    "    text = re.sub('\\&', \" and \", text)\n",
    "    \n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\W|^)([0-9]+)[kK](\\W|$)\", r\"\\1\\g<2>000\\3\", text) # better regex provided by @armamut\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "\n",
    "    # Original\n",
    "    text = re.sub(r\" u s \", \" America \", text)\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?U\\.S\\.A\\.\", \" America \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?United State(s)?\", \" America \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" (the[\\s]+|The[\\s]+)?US(A)? \", \" America \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" india \", \" India \", text)\n",
    "    text = re.sub(r\" switzerland \", \" Switzerland \", text)\n",
    "    text = re.sub(r\" china \", \" China \", text)\n",
    "    text = re.sub(r\" chinese \", \" Chinese \", text)\n",
    "    text = re.sub(r\" quora \", \" Quora \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" qoura \", \" Quora \", text)\n",
    "    text = re.sub(r\" upvote\", \" up vote\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" J K \", \" JK \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" J\\.K\\. \", \" JK \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(?=[a-zA-Z])ig \", \"ing \", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }    \n",
    "def clean_map_text(x):\n",
    "    for dic in [contraction_mapping, mispell_dict, punct_mapping]:\n",
    "        for word in dic.keys():\n",
    "            x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispell_dict = {'colour':'color',\n",
    "                'centre':'center',\n",
    "                'didnt':'did not',\n",
    "                'doesnt':'does not',\n",
    "                'isnt':'is not',\n",
    "                'shouldnt':'should not',\n",
    "                'favourite':'favorite',\n",
    "                'travelling':'traveling',\n",
    "                'counselling':'counseling',\n",
    "                'theatre':'theater',\n",
    "                'cancelled':'canceled',\n",
    "                'labour':'labor',\n",
    "                'organisation':'organization',\n",
    "                'wwii':'world war 2',\n",
    "                'citicise':'criticize',\n",
    "                'instagram': 'social medium',\n",
    "                'whatsapp': 'social medium',\n",
    "                'snapchat': 'social medium'\n",
    "                }\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How did Quebec nationalists see their province as a nation in the 1960s?', 'Do you have an adopted dog, how would you encourage people to adopt and not shop?', 'Why does velocity affect time? Does velocity affect space geometry?', 'How did Otto von Guericke used the Magdeburg hemispheres?', 'Can I convert montra helicon D to a mountain bike by just changing the tyres?', 'Is Gaza slowly becoming Auschwitz, Dachau or Treblinka for Palestinians?', 'Why does Quora automatically ban conservative opinions when reported, but does not do the same for liberal views?', 'Is it crazy if I wash or wipe my groceries off? Germs are everywhere.', 'Is there such a thing as dressing moderately, and if so, how is that different than dressing modestly?', 'Is it just me or have you ever been in this phase wherein you became ignorant to the people you once loved, completely disregarding their feelings/lives so you get to have something go your way and feel temporarily at ease. How did things change?']\n"
     ]
    }
   ],
   "source": [
    "def quara_load_data():\n",
    "    train = pd.read_csv('../input/train.csv')\n",
    "    test = pd.read_csv('../input/test.csv')\n",
    "    return train, test\n",
    "\n",
    "train, test = quara_load_data()\n",
    "# Load id Text List\n",
    "train_id_list = list(train[key].values)\n",
    "test_id_list = list(test[key].values)\n",
    "train_text_list = list(train[qt].values)\n",
    "test_text_list = list(test[qt].values)\n",
    "id_list = train_id_list + test_id_list\n",
    "text_list = train_text_list + test_text_list\n",
    "\n",
    "raw_trn_idx = list(train.index)\n",
    "raw_test_idx = list(test.index)\n",
    "y = train[target]\n",
    "\n",
    "del train, test\n",
    "gc.collect()\n",
    "\n",
    "print(train_text_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"Basic Feature Engineering\"):\n",
    "    train[\"num_words\"] = train[\"question_text\"].apply(lambda x: len(str(x).split()))\n",
    "    test[\"num_words\"] = test[\"question_text\"].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    ## Number of unique words in the text ## \n",
    "    train[\"num_unique_words\"] = train[\"question_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "    test[\"num_unique_words\"] = test[\"question_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "    \n",
    "    ## Number of characters in the text ##\n",
    "    train[\"num_chars\"] = train[\"question_text\"].apply(lambda x: len(str(x)))\n",
    "    test[\"num_chars\"] = test[\"question_text\"].apply(lambda x: len(str(x)))\n",
    "    \n",
    "    ## Number of stopwords in the text ##\n",
    "    train[\"num_stopwords\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w in STOPWORDS ]))\n",
    "    test[\"num_stopwords\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w in STOPWORDS ]))\n",
    "    \n",
    "    ## Number of punctuations in the text ##\n",
    "    train[\"num_punctuations\"] = train[\"question_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "    test[\"num_punctuations\"] = test[\"question_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "    \n",
    "    ## Number of title case words in the text ##\n",
    "    train[\"num_words_upper\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    test[\"num_words_upper\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    \n",
    "    ## Number of title case words in the text ##\n",
    "    train[\"num_words_upper\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    test[\"num_words_upper\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    \n",
    "    ## Number of title case words in the text ##\n",
    "    train[\"num_words_lower\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.islower()]))\n",
    "    test[\"num_words_lower\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.islower()]))\n",
    "    \n",
    "    ## Number of upper chars in the text ##\n",
    "    train[\"num_chars_upper\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x) if w.isupper()]))\n",
    "    test[\"num_chars_upper\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x) if w.isupper()]))\n",
    "    \n",
    "    ## Number of lower chars in the text ##\n",
    "    train[\"num_chars_lower\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x) if w.islower()]))\n",
    "    test[\"num_chars_lower\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x) if w.islower()]))\n",
    "    \n",
    "    ## Number of title case words in the text ##\n",
    "    train[\"num_words_title\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    test[\"num_words_title\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    \n",
    "    ## Average length of the words in the text ##\n",
    "    train[\"mean_word_len\"] = train[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    test[\"mean_word_len\"] = test[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    \n",
    "    ## Max length of the words in the text ##\n",
    "    train[\"max_word_len\"] = train[\"question_text\"].apply(lambda x: np.max([len(w) for w in str(x).split()]))\n",
    "    test[\"max_word_len\"] = test[\"question_text\"].apply(lambda x: np.max([len(w) for w in str(x).split()]))\n",
    "    \n",
    "    ## Min length of the words in the text ##\n",
    "    train[\"min_word_len\"] = train[\"question_text\"].apply(lambda x: np.min([len(w) for w in str(x).split()]))\n",
    "    test[\"min_word_len\"] = test[\"question_text\"].apply(lambda x: np.min([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The better written the code, the easier the copy pasta\n",
    "\n",
    "# Contraction replacement patterns\n",
    "cont_patterns = [\n",
    "    (b'(W|w)on\\'t', b'will not'),\n",
    "    (b'(C|c)an\\'t', b'can not'),\n",
    "    (b'(I|i)\\'m', b'i am'),\n",
    "    (b'(A|a)in\\'t', b'is not'),\n",
    "    (b'(\\w+)\\'ll', b'\\g<1> will'),\n",
    "    (b'(\\w+)n\\'t', b'\\g<1> not'),\n",
    "    (b'(\\w+)\\'ve', b'\\g<1> have'),\n",
    "    (b'(\\w+)\\'s', b'\\g<1> is'),\n",
    "    (b'(\\w+)\\'re', b'\\g<1> are'),\n",
    "    (b'(\\w+)\\'d', b'\\g<1> would'),\n",
    "]\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "\n",
    "def prepare_for_char_n_gram(text):\n",
    "    \"\"\" Simple text clean up process\"\"\"\n",
    "    # 1. Go to lower case (only good for english)\n",
    "    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n",
    "    clean = bytes(text.lower(), encoding=\"utf-8\")\n",
    "    # 2. Drop \\n and  \\t\n",
    "    clean = clean.replace(b\"\\n\", b\" \")\n",
    "    clean = clean.replace(b\"\\t\", b\" \")\n",
    "    clean = clean.replace(b\"\\b\", b\" \")\n",
    "    clean = clean.replace(b\"\\r\", b\" \")\n",
    "    # 3. Replace english contractions\n",
    "    for (pattern, repl) in patterns:\n",
    "        clean = re.sub(pattern, repl, clean)\n",
    "    # 4. Drop puntuation\n",
    "    # I could have used regex package with regex.sub(b\"\\p{P}\", \" \")\n",
    "    exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n",
    "    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n",
    "    # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n",
    "    clean = re.sub(b\"\\d+\", b\" \", clean)\n",
    "    # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n",
    "    clean = re.sub(b'\\s+', b' ', clean)\n",
    "    # Remove ending space if any\n",
    "    clean = re.sub(b'\\s+$', b'', clean)\n",
    "    # 7. Now replace words by words surrounded by # signs\n",
    "    # e.g. my name is bond would become #my# #name# #is# #bond#\n",
    "    # clean = re.sub(b\"([a-z]+)\", b\"#\\g<1>#\", clean)\n",
    "    clean = re.sub(b\" \", b\"# #\", clean)  # Replace space\n",
    "    clean = b\"#\" + clean + b\"#\"  # add leading and trailing #\n",
    "\n",
    "    return str(clean, 'utf-8')\n",
    "\n",
    "def count_regexp_occ(regexp=\"\", text=None):\n",
    "    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n",
    "    return len(re.findall(regexp, text))\n",
    "\n",
    "def get_indicators_and_clean_comments(df, text_var):\n",
    "    \"\"\"\n",
    "    Check all sorts of content as it may help find toxic comment\n",
    "    Though I'm not sure all of them improve scores\n",
    "    \"\"\"\n",
    "    # Count number of \\n\n",
    "    df[\"ant_slash_n\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\n\", x))\n",
    "    # Get length in words and characters\n",
    "    df[\"raw_word_len\"] = df[text_var].apply(lambda x: len(x.split()))\n",
    "    df[\"raw_char_len\"] = df[text_var].apply(lambda x: len(x))\n",
    "    # Check number of upper case, if you're angry you may write in upper case\n",
    "    df[\"nb_upper\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"[A-Z]\", x))\n",
    "    # Number of F words - f..k contains folk, fork,\n",
    "    df[\"nb_fk\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"[Ff]\\S{2}[Kk]\", x))\n",
    "    # Number of S word\n",
    "    df[\"nb_sk\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"[Ss]\\S{2}[Kk]\", x))\n",
    "    # Number of D words\n",
    "    df[\"nb_dk\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"[dD]ick\", x))\n",
    "    # Number of occurence of You, insulting someone usually needs someone called : you\n",
    "    df[\"nb_you\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\W[Yy]ou\\W\", x))\n",
    "    # Just to check you really refered to my mother ;-)\n",
    "    df[\"nb_mother\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\Wmother\\W\", x))\n",
    "    # Just checking for toxic 19th century vocabulary\n",
    "    df[\"nb_ng\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\Wnigger\\W\", x))\n",
    "    # Some Sentences start with a <:> so it may help\n",
    "    df[\"start_with_columns\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"^\\:+\", x))\n",
    "    # Check for time stamp\n",
    "    df[\"has_timestamp\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\d{2}|:\\d{2}\", x))\n",
    "    # Check for dates 18:44, 8 December 2010\n",
    "    df[\"has_date_long\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\D\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4}\", x))\n",
    "    # Check for date short 8 December 2010\n",
    "    df[\"has_date_short\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\D\\d{1,2} \\w+ \\d{4}\", x))\n",
    "    # Check for http links\n",
    "    df[\"has_http\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"http[s]{0,1}://\\S+\", x))\n",
    "    # check for mail\n",
    "    df[\"has_mail\"] = df[text_var].apply(\n",
    "        lambda x: count_regexp_occ(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', x)\n",
    "    )\n",
    "    # Looking for words surrounded by == word == or \"\"\"\" word \"\"\"\"\n",
    "    df[\"has_emphasize_equal\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\={2}.+\\={2}\", x))\n",
    "    df[\"has_emphasize_quotes\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\\"{4}\\S+\\\"{4}\", x))\n",
    "\n",
    "    # Now clean comments\n",
    "    df[\"clean_comment\"] = df[text_var].apply(lambda x: prepare_for_char_n_gram(x))\n",
    "\n",
    "    # Get the new length in words and characters\n",
    "    df[\"clean_word_len\"] = df[\"clean_comment\"].apply(lambda x: len(x.split()))\n",
    "    df[\"clean_char_len\"] = df[\"clean_comment\"].apply(lambda x: len(x))\n",
    "    # Number of different characters used in a comment\n",
    "    # Using the f word only will reduce the number of letters required in the comment\n",
    "    df[\"clean_chars\"] = df[\"clean_comment\"].apply(lambda x: len(set(x)))\n",
    "    df[\"clean_chars_ratio\"] = df[\"clean_comment\"].apply(lambda x: len(set(x))) / df[\"clean_comment\"].apply(\n",
    "        lambda x: 1 + min(99, len(x)))\n",
    "    \n",
    "    return df\n",
    "    \n",
    "with timer(\"Performing basic NLP\"):\n",
    "    train = get_indicators_and_clean_comments(train, 'question_text')\n",
    "    test = get_indicators_and_clean_comments(test,  'question_text')\n",
    "    \n",
    "    num_features = [f_ for f_ in train.columns\n",
    "                if f_ not in [\"question_text\", \"clean_comment\", \"remaining_chars\",\n",
    "                              'has_ip_address', 'target']]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cleansing Dataset] done in 64 s\n"
     ]
    }
   ],
   "source": [
    "# Cleansing\n",
    "with timer(\"Cleansing Dataset\"):\n",
    "    # 並列処理でクレンジング\n",
    "    train_text_list = Parallel(n_jobs=-1)( [delayed(cleansing_text)(args) for args in train_text_list] )\n",
    "    train_text_list = Parallel(n_jobs=-1)( [delayed(prepare_for_char_n_gram)(args) for args in train_text_list] )\n",
    "    train_text_list = Parallel(n_jobs=-1)( [delayed(clean_map_text)(args) for args in train_text_list] )\n",
    "    train_text_list = Parallel(n_jobs=-1)( [delayed(replace_typical_misspell)(args) for args in train_text_list] )\n",
    "    \n",
    "    test_text_list = Parallel(n_jobs=-1)( [delayed(cleansing_text)(args) for args in test_text_list] )\n",
    "    test_text_list = Parallel(n_jobs=-1)( [delayed(prepare_for_char_n_gram)(args) for args in test_text_list] )\n",
    "    test_text_list = Parallel(n_jobs=-1)( [delayed(clean_map_text)(args) for args in test_text_list] )\n",
    "    test_text_list = Parallel(n_jobs=-1)( [delayed(replace_typical_misspell)(args) for args in test_text_list] )\n",
    "    \n",
    "    text_list = train_text_list + test_text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_analyzer(text):\n",
    "    \"\"\"\n",
    "    This is used to split strings in small lots\n",
    "    I saw this in an article (I can't find the link anymore)\n",
    "    so <talk> and <talking> would have <Tal> <alk> in common\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    return [token[i: i + 3] for token in tokens for i in range(len(token) - 2)]\n",
    "\n",
    "all_text = pd.concat([train['question_text'],test['question_text']], axis =0)\n",
    "\n",
    "word_vect = TfidfVectorizer(\n",
    "            sublinear_tf=True,\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            token_pattern=r'\\w{1,}',\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=10,\n",
    "            max_df=0.5,\n",
    "            max_features=20000)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "            sublinear_tf=True,\n",
    "            strip_accents='unicode',\n",
    "            tokenizer=char_analyzer,\n",
    "            analyzer='word',\n",
    "            ngram_range=(1, 1),\n",
    "            min_df=10,\n",
    "            max_df=0.5,\n",
    "            max_features=50000)\n",
    "\n",
    "with timer(\"Word Grams TFIDF\"):\n",
    "    word_vect.fit(all_text)\n",
    "    train_word_features  = word_vect.transform(train['question_text'])\n",
    "    test_word_features  = word_vect.transform(test['question_text'])\n",
    "\n",
    "with timer(\"Character Grams TFIDF\"):\n",
    "    char_vectorizer.fit(all_text)\n",
    "    train_char_features = char_vectorizer.transform(train['question_text'])\n",
    "    test_char_features = char_vectorizer.transform(test['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Sparse Matrix Feature Names..\n",
    "feature_names = word_vect.get_feature_names() + char_vectorizer.get_feature_names() + num_features\n",
    "del all_text; gc.collect()\n",
    "\n",
    "with timer(\"Sparse Combine\"):\n",
    "    X = hstack(\n",
    "        [\n",
    "            train_char_features,\n",
    "            train_word_features,\n",
    "            train[num_features]\n",
    "        ]\n",
    "    ).tocsr()\n",
    "\n",
    "    del train_char_features\n",
    "    gc.collect()\n",
    "\n",
    "    testing = hstack(\n",
    "        [\n",
    "            test_char_features,\n",
    "            test_word_features,\n",
    "            test[num_features]\n",
    "        ]\n",
    "    ).tocsr()\n",
    "    del test_char_features; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_st_time = time.time()\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import log_loss, confusion_matrix, f1_score, accuracy_score\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from eli5.permutation_importance import get_score_importances\n",
    "\n",
    "start_time = \"{0:%Y%m%d_%H%M%S}\".format(datetime.datetime.now())\n",
    "\n",
    "\n",
    "def f1_calculation(y_val, y_pred):\n",
    "    for thresh in np.arange(0.1, 0.401, 0.01):\n",
    "        thresh = np.round(thresh, 2)\n",
    "        f1 = f1_score(y_val, (y_pred>thresh).astype(int))\n",
    "        logger.info(f\"F1 score at threshold {thresh} is {f1}\")\n",
    "        \n",
    "        \n",
    "# Memory Error\n",
    "def start_svd(df_tfidf, dim=1000):\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    svd = TruncatedSVD(n_components=dim, random_state=seed)\n",
    "    svd_tfidf = svd.fit_transform(df_tfidf)\n",
    "#     col_names = [f\"svd{dim}_tfidf_{i}\" for i in range(dim)]\n",
    "#     df_svd = pd.DataFrame(svd_tfidf, columns=col_names)\n",
    "    return svd_tfidf\n",
    "    \n",
    "\n",
    "def lgbm_train(train_test, feature_list, fold):\n",
    "\n",
    "    # LGBM Args\n",
    "    model_type = 'lgb'\n",
    "    fold_type = 'stratified'\n",
    "    metric = 'accuracy'\n",
    "#     fold = 5\n",
    "    learning_rate = 0.1\n",
    "    early_stopping_rounds = 100\n",
    "    num_boost_round = 10000\n",
    "    seed = 1208\n",
    "    params = {\n",
    "        'num_threads': -1,\n",
    "        'metric': 'binary_logloss',\n",
    "        'objective': 'binary',\n",
    "        'boosting_type':'gbdt',\n",
    "        'bagging_freq': 1,\n",
    "        'sigmoid': 1.1,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree': 0.2,\n",
    "        'lambda_l1': 1,\n",
    "        'lambda_l2': 5,\n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves': 100,\n",
    "        'max_depth': 9,\n",
    "        'bagging_seed': 1208,\n",
    "        'data_random_seed': 1208,\n",
    "        'feature_fraction_seed': 1208,\n",
    "        'random_seed': 1208,\n",
    "        'verbose': 1\n",
    "    }\n",
    "\n",
    "    # Result\n",
    "    select_features = [] # Feature Importanceによって選択したfeature群を入れるリスト\n",
    "    prediction = np.zeros(len(raw_trn_idx))\n",
    "\n",
    "    with timer(\"LGBM Setting\"):\n",
    "\n",
    "        # testも結合されてるので、trainのみにする\n",
    "#         tmp_train = csr_tfidf[raw_trn_idx]\n",
    "        tmp_train = train_test[raw_trn_idx]\n",
    "\n",
    "        ' KFold '\n",
    "        if fold_type == 'stratified':\n",
    "            folds = StratifiedKFold(n_splits=fold, shuffle=True, random_state=seed)\n",
    "            kfold = folds.split(tmp_train, y)\n",
    "\n",
    "    cv_feim = pd.DataFrame() # Feature Importanceの結果ファイルを入れるDF\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kfold):\n",
    "\n",
    "        with timer(f\"Validation: {n_fold} | LGBM Train\"):\n",
    "            x_train, y_train = tmp_train[trn_idx], y[trn_idx]\n",
    "            x_val, y_val = tmp_train[val_idx], y[val_idx]\n",
    "\n",
    "            # Dataset\n",
    "            lgb_train = lgb.Dataset(data=x_train, label=y_train)\n",
    "            lgb_eval = lgb.Dataset(data=x_val, label=y_val)\n",
    "\n",
    "            estimator = lgb.train(\n",
    "                train_set=lgb_train,\n",
    "                valid_sets=lgb_eval,\n",
    "                params=params,\n",
    "                verbose_eval=200,\n",
    "                early_stopping_rounds=early_stopping_rounds,\n",
    "                num_boost_round=num_boost_round\n",
    "            )\n",
    "            \n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         def score(X, y):\n",
    "#             y_pred = estimator.predict(X)\n",
    "#             return accuracy_score(y, y_pred)\n",
    "#         base_score, score_decreases = get_score_importances(score, x_train, y_train)\n",
    "#         feature_importances = np.mean(score_decreases, axis=0)\n",
    "        \n",
    "#         from matplotlib import pyplot as plt\n",
    "#         %matplotlib inline\n",
    "#         import seaborn as sns\n",
    "        \n",
    "#         plt.figure(figsize=(8, 12))\n",
    "#         sns.barplot(data=[feature_importances[:50], feature_list[:50]], x='avg_importance', y='feature')\n",
    "#         plt.show()\n",
    "        \n",
    "#         total_time = (time.time() - start_time)/60.0\n",
    "#         print(f\"Permutation Importance Calculate: {total_time}\")\n",
    "#         sys.exit()\n",
    "\n",
    "        with timer(f\"Validation: {n_fold} | Prediction & Get F1 score\"):\n",
    "            y_pred = estimator.predict(x_val)\n",
    "            score = log_loss(y_val, y_pred)\n",
    "            logger.info(f'Fold No: {n_fold} | {metric}: {score}')\n",
    "            logger.info(f\"Train Shape: {x_train.shape}\")\n",
    "            \n",
    "            prediction[val_idx] = y_pred\n",
    "            \n",
    "            f1_calculation(y_val, y_pred)\n",
    "\n",
    "            ' Feature Importance '\n",
    "            if len(cv_feim):\n",
    "                cv_feim[f'{n_fold}_importance'] = estimator.feature_importance(importance_type='gain')\n",
    "            else:\n",
    "                feim_name = f'{n_fold}_importance'\n",
    "                feim = pd.Series(estimator.feature_importance(importance_type='gain'), name=feim_name, index=feature_list).to_frame().reset_index().rename(columns={'index':'feature'})\n",
    "                cv_feim = feim.copy()\n",
    "                \n",
    "    f1_calculation(y, prediction)\n",
    "\n",
    "    with timer(\"Save Feature Importance\"):\n",
    "        col_feim = [col for col in cv_feim.columns if col.count('importance')]\n",
    "        cv_feim['avg_importance'] = cv_feim[col_feim].mean(axis=1)\n",
    "        cv_feim.sort_values(by='avg_importance', ascending=False, inplace=True)\n",
    "\n",
    "        from matplotlib import pyplot as plt\n",
    "        %matplotlib inline\n",
    "        import seaborn as sns\n",
    "        plt.figure(figsize=(8, 12))\n",
    "        display(cv_feim.head())\n",
    "        sns.barplot(data=cv_feim.iloc[:50, ], x='avg_importance', y='feature')\n",
    "        plt.show()\n",
    "        if is_local:\n",
    "            cv_feim.to_csv(f'../valid/{start_time[4:12]}_{model_type}_TFIDF_f1{f1}_logloss{score}_lr{learning_rate}.csv', index=False)\n",
    "        tmp_features = list(cv_feim[cv_feim['avg_importance']>30]['feature'].values)\n",
    "        print(f'Selece Feature: {len(tmp_features)}')\n",
    "        select_features += tmp_features\n",
    "\n",
    "    select_features = list(set(select_features))\n",
    "    print(len(select_features))\n",
    "    print(sorted(select_features)[:20])\n",
    "    print(f'All done in {time.time() - train_st_time:.0f} s')\n",
    "    \n",
    "    return select_features, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train = csr_tfidf\n",
    "select_features, _ = lgbm_train(tmp_train, feature_list, fold=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance 上位のFeature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2509\n",
      "2509\n",
      "(1362492, 2509)\n"
     ]
    }
   ],
   "source": [
    "id_list = []\n",
    "[id_list.append(id)  for word, id in tfidf_vocablary if word in select_features]\n",
    "print(len(id_list))\n",
    "print(len(select_features))\n",
    "# sparse matrixのcolumn No=id No\n",
    "select_mx = csr_tfidf.T[id_list].T\n",
    "print(select_mx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"Selected Train\"):\n",
    "    select_features, prediction = lgbm_train(select_mx, select_features, fold=5)\n",
    "    print(len(select_features))\n",
    "    print(select_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_st_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3e995d26e3a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'All Done!! {time.time() - train_st_time:.0f} s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_st_time' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'All Done!! {time.time() - st_time:.0f} s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
