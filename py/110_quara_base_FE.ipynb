{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:48:22.874288Z",
     "start_time": "2018-11-08T13:48:22.400005Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-27 08:49:51,314 utils 366 [INFO]    [logger_func] start \n",
      "2018-11-27 08:49:51,314 utils 366 [INFO]    [logger_func] start \n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import sys\n",
    "import re\n",
    "import gc\n",
    "import glob\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import os\n",
    "HOME = os.path.expanduser('~')\n",
    "sys.path.append(f\"{HOME}/kaggle/data_analysis/library/\")\n",
    "import utils\n",
    "from utils import logger_func, get_categorical_features, get_numeric_features, pararell_process\n",
    "logger = logger_func()\n",
    "pd.set_option('max_columns', 200)\n",
    "pd.set_option('max_rows', 200)\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    \"\"\"\n",
    "    Taken from Konstantin Lopuhin https://www.kaggle.com/lopuhin\n",
    "    in script named : Mercari Golf: 0.3875 CV in 75 LOC, 1900 s\n",
    "    https://www.kaggle.com/lopuhin/mercari-golf-0-3875-cv-in-75-loc-1900-s\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:48:26.393696Z",
     "start_time": "2018-11-08T13:48:25.902906Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.99it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 94.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# DatasetのPickle化\n",
    "# train = pd.read_csv('../input/train.csv')\n",
    "# test = pd.read_csv('../input/test.csv')\n",
    "# utils.to_df_pkl(df=train, path='../input', fname='train')\n",
    "# utils.to_df_pkl(df=test, path='../input', fname='test')\n",
    "train = utils.read_df_pkl(path='../input/train*.p')\n",
    "test = utils.read_df_pkl(path='../input/test*.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:57:20.934406Z",
     "start_time": "2018-11-08T13:57:19.919184Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:49:45.766908Z",
     "start_time": "2018-11-08T13:48:36.668417Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feature Engineering] done in 73 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Basic Feature Engineering\"):\n",
    "    train[\"num_words\"] = train[\"question_text\"].apply(lambda x: len(str(x).split()))\n",
    "    test[\"num_words\"] = test[\"question_text\"].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    ## Number of unique words in the text ## \n",
    "    train[\"num_unique_words\"] = train[\"question_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "    test[\"num_unique_words\"] = test[\"question_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "    \n",
    "    ## Number of characters in the text ##\n",
    "    train[\"num_chars\"] = train[\"question_text\"].apply(lambda x: len(str(x)))\n",
    "    test[\"num_chars\"] = test[\"question_text\"].apply(lambda x: len(str(x)))\n",
    "    \n",
    "    ## Number of stopwords in the text ##\n",
    "    train[\"num_stopwords\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w in STOPWORDS ]))\n",
    "    test[\"num_stopwords\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w in STOPWORDS ]))\n",
    "    \n",
    "    ## Number of punctuations in the text ##\n",
    "    train[\"num_punctuations\"] = train[\"question_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "    test[\"num_punctuations\"] = test[\"question_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "    \n",
    "    ## Number of title case words in the text ##\n",
    "    train[\"num_words_upper\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    test[\"num_words_upper\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    \n",
    "    ## Number of title case words in the text ##\n",
    "    train[\"num_words_upper\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    test[\"num_words_upper\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    \n",
    "    ## Number of title case words in the text ##\n",
    "    train[\"num_words_lower\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.islower()]))\n",
    "    test[\"num_words_lower\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.islower()]))\n",
    "    \n",
    "    ## Number of upper chars in the text ##\n",
    "    train[\"num_chars_upper\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x) if w.isupper()]))\n",
    "    test[\"num_chars_upper\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x) if w.isupper()]))\n",
    "    \n",
    "    ## Number of lower chars in the text ##\n",
    "    train[\"num_chars_lower\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x) if w.islower()]))\n",
    "    test[\"num_chars_lower\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x) if w.islower()]))\n",
    "    \n",
    "    ## Number of title case words in the text ##\n",
    "    train[\"num_words_title\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    test[\"num_words_title\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    \n",
    "    ## Average length of the words in the text ##\n",
    "    train[\"mean_word_len\"] = train[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    test[\"mean_word_len\"] = test[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    \n",
    "    ## Max length of the words in the text ##\n",
    "    train[\"max_word_len\"] = train[\"question_text\"].apply(lambda x: np.max([len(w) for w in str(x).split()]))\n",
    "    test[\"max_word_len\"] = test[\"question_text\"].apply(lambda x: np.max([len(w) for w in str(x).split()]))\n",
    "    \n",
    "    ## Min length of the words in the text ##\n",
    "    train[\"min_word_len\"] = train[\"question_text\"].apply(lambda x: np.min([len(w) for w in str(x).split()]))\n",
    "    test[\"min_word_len\"] = test[\"question_text\"].apply(lambda x: np.min([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Word Grams TFIDF] done in 57 s\n",
      "[Character Grams TFIDF] done in 58 s\n",
      "[Performing basic NLP] done in 151 s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hstack' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-151f26e976f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sparse Combine\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     X = hstack(\n\u001b[0m\u001b[1;32m    159\u001b[0m         [\n\u001b[1;32m    160\u001b[0m             \u001b[0mtrain_char_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hstack' is not defined"
     ]
    }
   ],
   "source": [
    "# The better written the code, the easier the copy pasta\n",
    "\n",
    "# Contraction replacement patterns\n",
    "cont_patterns = [\n",
    "    (b'(W|w)on\\'t', b'will not'),\n",
    "    (b'(C|c)an\\'t', b'can not'),\n",
    "    (b'(I|i)\\'m', b'i am'),\n",
    "    (b'(A|a)in\\'t', b'is not'),\n",
    "    (b'(\\w+)\\'ll', b'\\g<1> will'),\n",
    "    (b'(\\w+)n\\'t', b'\\g<1> not'),\n",
    "    (b'(\\w+)\\'ve', b'\\g<1> have'),\n",
    "    (b'(\\w+)\\'s', b'\\g<1> is'),\n",
    "    (b'(\\w+)\\'re', b'\\g<1> are'),\n",
    "    (b'(\\w+)\\'d', b'\\g<1> would'),\n",
    "]\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "\n",
    "def prepare_for_char_n_gram(text):\n",
    "    \"\"\" Simple text clean up process\"\"\"\n",
    "    # 1. Go to lower case (only good for english)\n",
    "    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n",
    "    clean = bytes(text.lower(), encoding=\"utf-8\")\n",
    "    # 2. Drop \\n and  \\t\n",
    "    clean = clean.replace(b\"\\n\", b\" \")\n",
    "    clean = clean.replace(b\"\\t\", b\" \")\n",
    "    clean = clean.replace(b\"\\b\", b\" \")\n",
    "    clean = clean.replace(b\"\\r\", b\" \")\n",
    "    # 3. Replace english contractions\n",
    "    for (pattern, repl) in patterns:\n",
    "        clean = re.sub(pattern, repl, clean)\n",
    "    # 4. Drop puntuation\n",
    "    # I could have used regex package with regex.sub(b\"\\p{P}\", \" \")\n",
    "    exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n",
    "    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n",
    "    # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n",
    "    clean = re.sub(b\"\\d+\", b\" \", clean)\n",
    "    # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n",
    "    clean = re.sub(b'\\s+', b' ', clean)\n",
    "    # Remove ending space if any\n",
    "    clean = re.sub(b'\\s+$', b'', clean)\n",
    "    # 7. Now replace words by words surrounded by # signs\n",
    "    # e.g. my name is bond would become #my# #name# #is# #bond#\n",
    "    # clean = re.sub(b\"([a-z]+)\", b\"#\\g<1>#\", clean)\n",
    "    clean = re.sub(b\" \", b\"# #\", clean)  # Replace space\n",
    "    clean = b\"#\" + clean + b\"#\"  # add leading and trailing #\n",
    "\n",
    "    return str(clean, 'utf-8')\n",
    "\n",
    "def count_regexp_occ(regexp=\"\", text=None):\n",
    "    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n",
    "    return len(re.findall(regexp, text))\n",
    "\n",
    "def get_indicators_and_clean_comments(df, text_var):\n",
    "    \"\"\"\n",
    "    Check all sorts of content as it may help find toxic comment\n",
    "    Though I'm not sure all of them improve scores\n",
    "    \"\"\"\n",
    "    # Count number of \\n\n",
    "    df[\"ant_slash_n\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\n\", x))\n",
    "    # Get length in words and characters\n",
    "    df[\"raw_word_len\"] = df[text_var].apply(lambda x: len(x.split()))\n",
    "    df[\"raw_char_len\"] = df[text_var].apply(lambda x: len(x))\n",
    "    # Check number of upper case, if you're angry you may write in upper case\n",
    "    df[\"nb_upper\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"[A-Z]\", x))\n",
    "    # Number of F words - f..k contains folk, fork,\n",
    "    df[\"nb_fk\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"[Ff]\\S{2}[Kk]\", x))\n",
    "    # Number of S word\n",
    "    df[\"nb_sk\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"[Ss]\\S{2}[Kk]\", x))\n",
    "    # Number of D words\n",
    "    df[\"nb_dk\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"[dD]ick\", x))\n",
    "    # Number of occurence of You, insulting someone usually needs someone called : you\n",
    "    df[\"nb_you\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\W[Yy]ou\\W\", x))\n",
    "    # Just to check you really refered to my mother ;-)\n",
    "    df[\"nb_mother\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\Wmother\\W\", x))\n",
    "    # Just checking for toxic 19th century vocabulary\n",
    "    df[\"nb_ng\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\Wnigger\\W\", x))\n",
    "    # Some Sentences start with a <:> so it may help\n",
    "    df[\"start_with_columns\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"^\\:+\", x))\n",
    "    # Check for time stamp\n",
    "    df[\"has_timestamp\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\d{2}|:\\d{2}\", x))\n",
    "    # Check for dates 18:44, 8 December 2010\n",
    "    df[\"has_date_long\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\D\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4}\", x))\n",
    "    # Check for date short 8 December 2010\n",
    "    df[\"has_date_short\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\D\\d{1,2} \\w+ \\d{4}\", x))\n",
    "    # Check for http links\n",
    "    df[\"has_http\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"http[s]{0,1}://\\S+\", x))\n",
    "    # check for mail\n",
    "    df[\"has_mail\"] = df[text_var].apply(\n",
    "        lambda x: count_regexp_occ(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', x)\n",
    "    )\n",
    "    # Looking for words surrounded by == word == or \"\"\"\" word \"\"\"\"\n",
    "    df[\"has_emphasize_equal\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\={2}.+\\={2}\", x))\n",
    "    df[\"has_emphasize_quotes\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\\"{4}\\S+\\\"{4}\", x))\n",
    "\n",
    "    # Now clean comments\n",
    "    df[\"clean_comment\"] = df[text_var].apply(lambda x: prepare_for_char_n_gram(x))\n",
    "\n",
    "    # Get the new length in words and characters\n",
    "    df[\"clean_word_len\"] = df[\"clean_comment\"].apply(lambda x: len(x.split()))\n",
    "    df[\"clean_char_len\"] = df[\"clean_comment\"].apply(lambda x: len(x))\n",
    "    # Number of different characters used in a comment\n",
    "    # Using the f word only will reduce the number of letters required in the comment\n",
    "    df[\"clean_chars\"] = df[\"clean_comment\"].apply(lambda x: len(set(x)))\n",
    "    df[\"clean_chars_ratio\"] = df[\"clean_comment\"].apply(lambda x: len(set(x))) / df[\"clean_comment\"].apply(\n",
    "        lambda x: 1 + min(99, len(x)))\n",
    "    \n",
    "def char_analyzer(text):\n",
    "    \"\"\"\n",
    "    This is used to split strings in small lots\n",
    "    I saw this in an article (I can't find the link anymore)\n",
    "    so <talk> and <talking> would have <Tal> <alk> in common\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    return [token[i: i + 3] for token in tokens for i in range(len(token) - 2)]\n",
    "\n",
    "all_text = pd.concat([train['question_text'],test['question_text']], axis =0)\n",
    "\n",
    "word_vect = TfidfVectorizer(\n",
    "            sublinear_tf=True,\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            token_pattern=r'\\w{1,}',\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=20000)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "            sublinear_tf=True,\n",
    "            strip_accents='unicode',\n",
    "            tokenizer=char_analyzer,\n",
    "            analyzer='word',\n",
    "            ngram_range=(1, 1),\n",
    "            max_features=50000)\n",
    "\n",
    "with timer(\"Word Grams TFIDF\"):\n",
    "    word_vect.fit(all_text)\n",
    "    train_word_features  = word_vect.transform(train['question_text'])\n",
    "    test_word_features  = word_vect.transform(test['question_text'])\n",
    "\n",
    "with timer(\"Character Grams TFIDF\"):\n",
    "    char_vectorizer.fit(all_text)\n",
    "    train_char_features = char_vectorizer.transform(train['question_text'])\n",
    "    test_char_features = char_vectorizer.transform(test['question_text'])\n",
    "\n",
    "with timer(\"Performing basic NLP\"):\n",
    "    get_indicators_and_clean_comments(train, 'question_text')\n",
    "    get_indicators_and_clean_comments(test,  'question_text')\n",
    "    \n",
    "    num_features = [f_ for f_ in train.columns\n",
    "                if f_ not in [\"question_text\", \"clean_comment\", \"remaining_chars\",\n",
    "                              'has_ip_address', 'target']]\n",
    "    \n",
    "# Get Sparse Matrix Feature Names..\n",
    "feature_names = word_vect.get_feature_names() + char_vectorizer.get_feature_names() + num_features\n",
    "del all_text; gc.collect()\n",
    "\n",
    "with timer(\"Sparse Combine\"):\n",
    "    X = hstack(\n",
    "        [\n",
    "            train_char_features,\n",
    "            train_word_features,\n",
    "            train[num_features]\n",
    "        ]\n",
    "    ).tocsr()\n",
    "\n",
    "    del train_char_features\n",
    "    gc.collect()\n",
    "\n",
    "    testing = hstack(\n",
    "        [\n",
    "            test_char_features,\n",
    "            test_word_features,\n",
    "            test[num_features]\n",
    "        ]\n",
    "    ).tocsr()\n",
    "    del test_char_features; gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
