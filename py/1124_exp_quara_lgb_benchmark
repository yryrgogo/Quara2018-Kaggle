{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is the purpose of this experiment?\n",
    "LGBMをアンサンブルの1つとするにあたり、ベンチマークを作成する。\n",
    "F1 Scoreと学習に使う時間を確認する。\n",
    "また、TFIDFとEmbeddingの2通りを試す\n",
    "### 2. Why do you this?\n",
    "多様なモデルを作る為\n",
    "### 3. Where are the points of technology and techniques?\n",
    "LGBM, TFIDF or Word Embedding\n",
    "### 4. How do you validate the effectiveness?\n",
    "処理時間\n",
    "F1 Score\n",
    "### 5. What will you do next?\n",
    "NNとアンサンブルしてLBを確認する。学習時間を確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T09:53:43.606080Z",
     "start_time": "2018-11-10T09:53:43.430947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "is_local = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import datetime\n",
    "import sys\n",
    "import re\n",
    "import gc\n",
    "import glob\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "def pararell_process(func, arg_list, cpu_cnt=multiprocessing.cpu_count()):\n",
    "    process = Pool(cpu_cnt)\n",
    "    callback = process.map_async(func, arg_list).get(600)\n",
    "    process.close()\n",
    "    process.terminate()\n",
    "    return callback\n",
    "\n",
    "def mkdir_func(path):\n",
    "    try:\n",
    "        os.stat(path)\n",
    "    except:\n",
    "        os.mkdir(path)\n",
    "\n",
    "from logging import StreamHandler, DEBUG, Formatter, FileHandler, getLogger\n",
    "def logger_func():\n",
    "    logger = getLogger(__name__)\n",
    "    log_fmt = Formatter('%(asctime)s %(name)s %(lineno)d [%(levelname)s]\\\n",
    "    [%(funcName)s] %(message)s ')\n",
    "    handler = StreamHandler()\n",
    "    handler.setLevel('INFO')\n",
    "    handler.setFormatter(log_fmt)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    mkdir_func('../output')\n",
    "    handler = FileHandler('../output/py_train.py.log', 'a')\n",
    "    handler.setLevel(DEBUG)\n",
    "    handler.setFormatter(log_fmt)\n",
    "    logger.setLevel(DEBUG)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    logger.info('start')\n",
    "\n",
    "    return logger\n",
    "logger = logger_func()\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    \"\"\"\n",
    "    Taken from Konstantin Lopuhin https://www.kaggle.com/lopuhin\n",
    "    in script named : Mercari Golf: 0.3875 CV in 75 LOC, 1900 s\n",
    "    https://www.kaggle.com/lopuhin/mercari-golf-0-3875-cv-in-75-loc-1900-s\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "    \n",
    "# NLP\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import string\n",
    "\n",
    "key = 'qid'\n",
    "qt = 'question_text'\n",
    "target = 'target'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contraction replacement patterns\n",
    "cont_patterns = [\n",
    "    (b'(W|w)on\\'t', b'will not'),\n",
    "    (b'(C|c)an\\'t', b'can not'),\n",
    "    (b'(I|i)\\'m', b'i am'),\n",
    "    (b'(A|a)in\\'t', b'is not'),\n",
    "    (b'(\\w+)\\'ll', b'\\g<1> will'),\n",
    "    (b'(\\w+)n\\'t', b'\\g<1> not'),\n",
    "    (b'(\\w+)\\'ve', b'\\g<1> have'),\n",
    "    (b'(\\w+)\\'s', b'\\g<1> is'),\n",
    "    (b'(\\w+)\\'re', b'\\g<1> are'),\n",
    "    (b'(\\w+)\\'d', b'\\g<1> would'),\n",
    "]\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "\n",
    "def prepare_for_char_n_gram(text):\n",
    "    \"\"\" Simple text clean up process\"\"\"\n",
    "    # 1. Go to lower case (only good for english)\n",
    "    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n",
    "    clean = bytes(text.lower(), encoding=\"utf-8\")\n",
    "    # 2. Drop \\n and  \\t\n",
    "    clean = clean.replace(b\"\\n\", b\" \")\n",
    "    clean = clean.replace(b\"\\t\", b\" \")\n",
    "    clean = clean.replace(b\"\\b\", b\" \")\n",
    "    clean = clean.replace(b\"\\r\", b\" \")\n",
    "    # 3. Replace english contractions\n",
    "    for (pattern, repl) in patterns:\n",
    "        clean = re.sub(pattern, repl, clean)\n",
    "    # 4. Drop puntuation\n",
    "    # I could have used regex package with regex.sub(b\"\\p{P}\", \" \")\n",
    "    exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n",
    "    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n",
    "    # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n",
    "    clean = re.sub(b\"\\d+\", b\" \", clean)\n",
    "    # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n",
    "    clean = re.sub(b'\\s+', b' ', clean)\n",
    "    # Remove ending space if any\n",
    "    clean = re.sub(b'\\s+$', b'', clean)\n",
    "    # 7. Now replace words by words surrounded by # signs\n",
    "    # e.g. my name is bond would become #my# #name# #is# #bond#\n",
    "    # clean = re.sub(b\"([a-z]+)\", b\"#\\g<1>#\", clean)\n",
    "#     clean = re.sub(b\" \", b\"# #\", clean)  # Replace space\n",
    "#     clean = b\"#\" + clean + b\"#\"  # add leading and trailing #\n",
    "\n",
    "    return str(clean, 'utf-8')\n",
    "\n",
    "def count_regexp_occ(regexp=\"\", text=None):\n",
    "    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n",
    "    return len(re.findall(regexp, text))\n",
    "\n",
    "def cleansing_text(text, remove_stopwords=True):\n",
    "\n",
    "    # Convert words to lower case and split them\n",
    "    text = re.sub(\"_\", \" \", text, flags=re.IGNORECASE)\n",
    "    text = text.lower().split()\n",
    "    regex_num = re.compile(u\"[0-9０-９]\")\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = STOPWORDS\n",
    "        text = [w for w in text if (not w in stops) and not(regex_num.match(w))]\n",
    "\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"[c-fC-F]\\:\\/\", \" disk \", text)\n",
    "    text = re.sub(\"\\'d\", \" would \", text)\n",
    "    text = re.sub(\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(\"\\'re\", \" are \", text)\n",
    "    text = re.sub(\"\\'s\", \" \", text) # we have cases like \"Sam is\" or \"Sam's\" (i.e. his) these two cases aren't separable, I choose to compromise are kill \"'s\" directly\n",
    "    text = re.sub(\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(\"\\(s\\)\", \" \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"`\", \"'\", text) # special single quote\n",
    "    text = re.sub(\"e-mail\", \" email \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(\"b\\.g\\.\", \" bg \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"e\\.g\\.\", \" eg \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"i'm\", \"i am\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"é\", \"e\", text)\n",
    "    text = re.sub(\"’\", \"'\", text) # special single quote\n",
    "    text = re.sub(\"“\", '\"', text) # special double quote\n",
    "    text = re.sub(\"…\", \" \", text)\n",
    "    text = re.sub(\"？\", \"?\", text)\n",
    "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
    "    text = re.sub('\\$', \" dollar \", text)\n",
    "    text = re.sub('\\%', \" percent \", text)\n",
    "    text = re.sub('\\&', \" and \", text)\n",
    "    \n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\W|^)([0-9]+)[kK](\\W|$)\", r\"\\1\\g<2>000\\3\", text) # better regex provided by @armamut\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "\n",
    "    # Original\n",
    "    text = re.sub(r\" u s \", \" America \", text)\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?U\\.S\\.A\\.\", \" America \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?United State(s)?\", \" America \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" (the[\\s]+|The[\\s]+)?US(A)? \", \" America \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" india \", \" India \", text)\n",
    "    text = re.sub(r\" switzerland \", \" Switzerland \", text)\n",
    "    text = re.sub(r\" china \", \" China \", text)\n",
    "    text = re.sub(r\" chinese \", \" Chinese \", text)\n",
    "    text = re.sub(r\" quora \", \" Quora \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" qoura \", \" Quora \", text)\n",
    "    text = re.sub(r\" upvote\", \" up vote\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" J K \", \" JK \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" J\\.K\\. \", \" JK \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(?=[a-zA-Z])ig \", \"ing \", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How did Quebec nationalists see their province as a nation in the 1960s?', 'Do you have an adopted dog, how would you encourage people to adopt and not shop?', 'Why does velocity affect time? Does velocity affect space geometry?', 'How did Otto von Guericke used the Magdeburg hemispheres?', 'Can I convert montra helicon D to a mountain bike by just changing the tyres?', 'Is Gaza slowly becoming Auschwitz, Dachau or Treblinka for Palestinians?', 'Why does Quora automatically ban conservative opinions when reported, but does not do the same for liberal views?', 'Is it crazy if I wash or wipe my groceries off? Germs are everywhere.', 'Is there such a thing as dressing moderately, and if so, how is that different than dressing modestly?', 'Is it just me or have you ever been in this phase wherein you became ignorant to the people you once loved, completely disregarding their feelings/lives so you get to have something go your way and feel temporarily at ease. How did things change?']\n"
     ]
    }
   ],
   "source": [
    "def quara_load_data():\n",
    "    train = pd.read_csv('../input/train.csv')\n",
    "    test = pd.read_csv('../input/test.csv')\n",
    "    return train, test\n",
    "\n",
    "train, test = quara_load_data()\n",
    "# Load id Text List\n",
    "train_id_list = list(train[key].values)\n",
    "test_id_list = list(test[key].values)\n",
    "train_text_list = list(train[qt].values)\n",
    "test_text_list = list(test[qt].values)\n",
    "id_list = train_id_list + test_id_list\n",
    "text_list = train_text_list + test_text_list\n",
    "\n",
    "raw_trn_idx = list(train.index)\n",
    "raw_test_idx = list(test.index)\n",
    "y = train[target]\n",
    "\n",
    "del train, test\n",
    "gc.collect()\n",
    "\n",
    "print(train_text_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cleansing Dataset] done in 56 s\n"
     ]
    }
   ],
   "source": [
    "# Cleansing\n",
    "with timer(\"Cleansing Dataset\"):\n",
    "    # 並列処理でクレンジング\n",
    "    train_text_list = Parallel(n_jobs=-1)( [delayed(cleansing_text)(args) for args in train_text_list] )\n",
    "    train_text_list = Parallel(n_jobs=-1)( [delayed(prepare_for_char_n_gram)(args) for args in train_text_list] )\n",
    "    test_text_list = Parallel(n_jobs=-1)( [delayed(cleansing_text)(args) for args in test_text_list] )\n",
    "    test_text_list = Parallel(n_jobs=-1)( [delayed(prepare_for_char_n_gram)(args) for args in test_text_list] )\n",
    "    text_list = train_text_list + test_text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF\n",
    "ngramはいくつかパターンを試す\n",
    "実験のため、tfidf matrixとvecotorizerは保存しておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fitting TFIDF] done in 11 s\n",
      "[Transform TFIDF] done in 10 s\n"
     ]
    }
   ],
   "source": [
    "ngram_range=(1,1)\n",
    "# ngram_range=(1,2)\n",
    "\n",
    "def get_tfidf(text_list):\n",
    "    '''\n",
    "    Explain:\n",
    "        テキストを1要素としてもったリストを渡し、TFIDFベクトルをもったsparse matrixを出力する\n",
    "    Args:\n",
    "        text_list(list): split前のテキストリスト. 1テキストが1つのTFIDFベクトルに変換される\n",
    "    Return:\n",
    "        sparse csr_matrix: TFIDF値が入ったスパースな行列\n",
    "    '''\n",
    "    # Get the tfidf\n",
    "    with timer(\"Fitting TFIDF\"):\n",
    "        # TFIDFに変換するオブジェクトを作成する\n",
    "        tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features = 10000,\n",
    "            min_df=10,\n",
    "            max_df=0.5,\n",
    "            stop_words=\"english\",\n",
    "            analyzer='word',\n",
    "            #  analyzer='char',\n",
    "            strip_accents='unicode',\n",
    "            ngram_range=ngram_range,\n",
    "            use_idf=True,\n",
    "            smooth_idf=True,\n",
    "            sublinear_tf=True\n",
    "        ).fit(text_list)\n",
    "\n",
    "    # Train, Testのテキストが順番に並んだリストを渡すと、各テキストをTFIDFベクトルに変換して返してくれる\n",
    "    # テキストに対応するインデックスを渡して返ってくるのが対応するTFIDFベクトル\n",
    "    with timer(\"Transform TFIDF\"):\n",
    "        csr_tfidf = tfidf_vectorizer.transform(text_list)\n",
    "    return csr_tfidf, tfidf_vectorizer\n",
    "\n",
    "# TFIDF\n",
    "csr_tfidf, tfidf_vectorizer = get_tfidf(text_list)\n",
    "\n",
    "# Kernel実行の場合は分岐\n",
    "if is_local:\n",
    "    HOME = os.path.expanduser(\"~\")\n",
    "    sys.path.append(f\"{HOME}/kaggle/data_analysis/library/\")\n",
    "    import utils\n",
    "    utils.to_pkl_gzip(obj=tfidf_vectorizer, path='../input/bench_vectorizer_tfidf30000.gz')\n",
    "    utils.to_pkl_gzip(obj=csr_tfidf, path='../input/bench_csr_tfidf30000.gz')\n",
    "    sys.path.append(f'{HOME}/kaggle/data_analysis/model')\n",
    "    from params_lgbm import params_quara\n",
    "else:\n",
    "    del text_list\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1de2b5a1d873>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# testも結合されてるので、trainのみにする\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#     tmp_train = csr_tfidf[raw_trn_idx]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mtmp_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mraw_trn_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;34m' KFold '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mcheck_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m         csr_sample_values(self.shape[0], self.shape[1],\n\u001b[1;32m    349\u001b[0m                           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "st_time = time.time()\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import log_loss, confusion_matrix, f1_score, accuracy_score\n",
    "start_time = \"{0:%Y%m%d_%H%M%S}\".format(datetime.datetime.now())\n",
    "\n",
    "def f1_calculation(y_val, y_pred):\n",
    "    for thresh in np.arange(0.1, 0.401, 0.01):\n",
    "        thresh = np.round(thresh, 2)\n",
    "        f1 = f1_score(y_val, (y_pred>thresh).astype(int))\n",
    "        logger.info(f\"F1 score at threshold {thresh} is {f1}\")\n",
    "        \n",
    "\n",
    "# Variables\n",
    "tfidf_vocablary = tfidf_vectorizer.vocabulary_.items()\n",
    "feature_list = np.array(tfidf_vectorizer.get_feature_names())\n",
    "del tfidf_vectorizer\n",
    "gc.collect()\n",
    "\n",
    "# LGBM Args\n",
    "model_type = 'lgb'\n",
    "fold_type = 'stratified'\n",
    "metric = 'accuracy'\n",
    "fold = 2\n",
    "learning_rate = 0.1\n",
    "early_stopping_rounds = 100\n",
    "num_boost_round = 10000\n",
    "seed = 1208\n",
    "params = {\n",
    "    'num_threads': -1,\n",
    "    'metric': 'binary_logloss',\n",
    "    'objective': 'binary',\n",
    "    'boosting_type':'gbdt',\n",
    "    'bagging_freq': 1,\n",
    "    'sigmoid': 1.1,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.2,\n",
    "    'lambda_l1': 1,\n",
    "    'lambda_l2': 5,\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 100,\n",
    "    'max_depth': 9,\n",
    "    'bagging_seed': 1208,\n",
    "    'data_random_seed': 1208,\n",
    "    'feature_fraction_seed': 1208,\n",
    "    'random_seed': 1208,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "# Result\n",
    "select_features = [] # Feature Importanceによって選択したfeature群を入れるリスト\n",
    "\n",
    "with timer(\"LGBM Setting\"):\n",
    "    \n",
    "    # testも結合されてるので、trainのみにする\n",
    "    tmp_train = csr_tfidf[raw_trn_idx]\n",
    "\n",
    "    ' KFold '\n",
    "    if fold_type == 'stratified':\n",
    "        folds = StratifiedKFold(n_splits=fold, shuffle=True, random_state=seed)\n",
    "        kfold = folds.split(tmp_train, y)\n",
    "    \n",
    "cv_feim = pd.DataFrame() # Feature Importanceの結果ファイルを入れるDF\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(kfold):\n",
    "    \n",
    "    with timer(f\"Validation: {n_fold} | LGBM Train\"):\n",
    "        x_train, y_train = tmp_train[trn_idx], y[trn_idx]\n",
    "        x_val, y_val = tmp_train[val_idx], y[val_idx]\n",
    "    \n",
    "        if n_fold==0:\n",
    "            prediction = np.zeros(len(x_val))\n",
    "        \n",
    "        # Dataset\n",
    "        lgb_train = lgb.Dataset(data=x_train, label=y_train)\n",
    "        lgb_eval = lgb.Dataset(data=x_val, label=y_val)\n",
    "        \n",
    "        estimator = lgb.train(\n",
    "            train_set=lgb_train,\n",
    "            valid_sets=lgb_eval,\n",
    "            params=params,\n",
    "            verbose_eval=200,\n",
    "            early_stopping_rounds=early_stopping_rounds,\n",
    "            num_boost_round=num_boost_round\n",
    "        )\n",
    "    \n",
    "    with timer(f\"Validation: {n_fold} | Prediction & Get F1 score\"):\n",
    "        y_pred = estimator.predict(x_val)\n",
    "        score = log_loss(y_val, y_pred)\n",
    "        logger.info(f'Fold No: {n_fold} | {metric}: {score}')\n",
    "        logger.info(f\"Train Shape: {x_train.shape}\")\n",
    "        \n",
    "        prediction += y_pred\n",
    "        \n",
    "        f1_calculation(y_val, y_pred)\n",
    "            \n",
    "        ' Feature Importance '\n",
    "        if len(cv_feim):\n",
    "            cv_feim[f'{n_fold}_importance'] = estimator.feature_importance(importance_type='gain')\n",
    "        else:\n",
    "            feim_name = f'{n_fold}_importance'\n",
    "            feim = pd.Series(estimator.feature_importance(importance_type='gain'), name=feim_name, index=feature_list).to_frame().reset_index().rename(columns={'index':'feature'})\n",
    "            cv_feim = feim.copy()\n",
    "            \n",
    "prediction /= n_fold+1\n",
    "f1_calculation(y_val, prediction)\n",
    "    \n",
    "with timer(\"Save Feature Importance\"):\n",
    "    col_feim = [col for col in cv_feim.columns if col.count('importance')]\n",
    "    cv_feim['avg_importance'] = cv_feim[col_feim].mean(axis=1)\n",
    "    cv_feim.sort_values(by='avg_importance', ascending=False, inplace=True)\n",
    "    \n",
    "    from matplotlib import pyplot as plt\n",
    "    import japanize_matplotlib\n",
    "    %matplotlib inline\n",
    "    import seaborn as sns\n",
    "    plt.figure(figsize=(8, 12))\n",
    "    display(cv_feim.head())\n",
    "    sns.barplot(data=cv_feim.iloc[:50, ], x='avg_importance', y='feature')\n",
    "    plt.show()\n",
    "    if is_local:\n",
    "        cv_feim.to_csv(f'../valid/{start_time[4:12]}_{model_type}_TFIDF_f1{f1}_logloss{score}_lr{learning_rate}.csv', index=False)\n",
    "    tmp_features = list(cv_feim[cv_feim['avg_importance']>30]['feature'].values)\n",
    "    print(f'Selece Feature: {len(tmp_features)}')\n",
    "    select_features += tmp_features\n",
    "        \n",
    "select_features = list(set(select_features))\n",
    "print(len(select_features))\n",
    "print(sorted(select_features))\n",
    "print(f'All done in {time.time() - st_time:.0f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance 上位のFeature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_list = []\n",
    "[id_list.append(id)  for word, id in tfidf_vocablary if word in select_features]\n",
    "print(len(id_list))\n",
    "print(len(select_features))\n",
    "# sparse matrixのcolumn No=id No\n",
    "select_mx = csr_tfidf.T[id_list].T\n",
    "print(select_mx.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
