{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP\n",
    "使用するライブラリやメソッドを読み込む  \n",
    "グローバル変数を宣言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import re\n",
    "import glob\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "from gensim import corpora, matutils\n",
    "HOME = os.path.expanduser('~')\n",
    "\n",
    "# Original Library\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/library/')\n",
    "import utils\n",
    "from pararell_utils import pararell_process\n",
    "\n",
    "# NLP Library\n",
    "from nlp_utils import stems, corpus_word_id\n",
    "from wordnet import search_similar_words\n",
    "\n",
    "logger = utils.logger_func()\n",
    "start_time = \"{0:%Y%m%d_%H%M%S}\".format(datetime.datetime.now())\n",
    "pd.set_option('max_columns', 200)\n",
    "pd.set_option('max_rows', 200)\n",
    "\n",
    "def get_pararell_stems(args):\n",
    "    return stems(**args)\n",
    "\n",
    "\n",
    "def get_pararell_words_dict(i, tx):\n",
    "    tmp_dict = {}\n",
    "    words_list = []\n",
    "    arg_list = []\n",
    "    for morph in morph_list:\n",
    "        arg_list.append({'text':tx, 'morph':morph, 'regex':True})\n",
    "    stem_list = pararell_process(get_pararell_stems, arg_list)\n",
    "    word_list = list(set(list(chain.from_iterable(stem_list))))\n",
    "\n",
    "    tmp_dict[i] = word_list\n",
    "\n",
    "    return tmp_dict\n",
    "\n",
    "#========================================================================\n",
    "# Global Variables\n",
    "#========================================================================\n",
    "key = 'unique_id'\n",
    "target = 'target'\n",
    "ignore_list = [key, target]\n",
    "regex_abcABC123 = re.compile(u\"[0-9０-９a-zA-Z]\")\n",
    "morph_list = ['名詞', '形容詞', '動詞']\n",
    "#  morph_list = ['名詞']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2015\n",
    "train = pd.read_excel(f'../input/jr_train{year}.xls')[['unique_id', 'contents', 'date', 'target']].dropna()\n",
    "train_2016 = pd.read_excel('../input/jr_train2016.xls')[['unique_id', 'contents', 'date', 'target']].dropna()\n",
    "train = pd.concat([train, train_2016], axis=0)\n",
    "test = pd.read_excel('../input/jr_test2017.xls')[['unique_id', 'contents', 'date', 'target']].dropna()\n",
    "train[key] = np.arange(len(train))\n",
    "test[key] = np.arange(len(test))\n",
    "\n",
    "print(train.shape)\n",
    "print(train.head())\n",
    "print(test.shape)\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**unique_id別にテキストを形態素解析にかけ、指定した品詞の単語リストを作成する**  \n",
    "**合わせて全テキストでの単語カウントを行う**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T08:52:30.161942Z",
     "start_time": "2018-11-15T08:52:30.116520Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8c8a407ce67a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_stems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mid_word_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "save = True\n",
    "\n",
    "def get_stems(df):\n",
    "    id_word_dict = {}\n",
    "    #========================================================================\n",
    "    # dictにunique_id別の単語リストが格納される\n",
    "    #========================================================================\n",
    "    tx_list = df['contents'].values\n",
    "    id_list = df['unique_id'].values\n",
    "    for i, tx in zip(id_list, tx_list):\n",
    "        logger.info(i)\n",
    "        id_word_dict.update(get_pararell_words_dict(i, tx))\n",
    "        \n",
    "    return id_word_dict\n",
    "    \n",
    "\"\"\" ここで作成したtrain_word_dictは後で使用する \"\"\"\n",
    "train_word_dict = get_stems(train)\n",
    "test_word_dict = get_stems(test)\n",
    "utils.to_pkl_gzip(obj=train_word_dict, path=f'../input/train_word_split_{year}_2016')\n",
    "utils.to_pkl_gzip(obj=test_word_dict, path=f'../input/test_word_split_2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 単語の出現頻度をカウントして保存→これを見てキーワードを選別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save=False\n",
    "train_word_dict = utils.read_pkl_gzip(path=f'../input/train_word_split_{year}.gz')\n",
    "test_word_dict = utils.read_pkl_gzip(path=f'../input/test_word_split_2017.gz')\n",
    "\n",
    "id_list = []\n",
    "tmp_list = []\n",
    "result = pd.DataFrame()\n",
    "for ui, texts in train_word_dict.items():\n",
    "    tmp = pd.Series(texts, name='word').to_frame()\n",
    "    tmp['unique_id'] = ui\n",
    "    if len(result):\n",
    "        result = pd.concat([result, tmp], axis=0)\n",
    "    else:\n",
    "        result = tmp.copy()\n",
    "id_word = result.drop_duplicates()\n",
    "# df_cnt = result.groupby('word').size().reset_index().rename(columns={0:'word_freq'}).sort_values(by='word_freq', ascending=False)\n",
    "# df_cnt = result.merge(id_word, on='word', how='inner').merge(train[[key, target]], on=key, how='inner')\n",
    "df_cnt = result.merge(train[[key, target]], on=key, how='inner')\n",
    "if save:\n",
    "    df_cnt.to_csv(f'../output/{start_time[:11]}_jrw_train_word_count.csv', index=False)\n",
    "\n",
    "logger.info(f\"\\nComplete Get Words From Texts!!\")\n",
    "display(df_cnt.shape)\n",
    "display(df_cnt.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 作成したキーワードリストを読み込む\n",
    "#### 単語リストをキーワードのみに絞る\n",
    "#### 前の実行部分で取得したid_word_dictを使用しないとunique_idと対応させるのが手間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyword_list(keyword_path):\n",
    "    #========================================================================\n",
    "    # 読み込みキーワードリストはcsvでヘッダーなしの1行を想定\n",
    "    #========================================================================\n",
    "    keyword_path = glob.glob(keyword_path)\n",
    "    keyword_list = []\n",
    "    for path in keyword_path:\n",
    "        tmp = list(pd.read_csv(path, header=None).iloc[:, 0].values)\n",
    "        keyword_list += tmp\n",
    "    keyword_list = list(set(keyword_list))\n",
    "    return keyword_list\n",
    "\n",
    "def keyword_filter(id_word_dict, keyword_list):\n",
    "    id_keyword_dict = {}\n",
    "    for i, word_list in tqdm(id_word_dict.items()):\n",
    "        new_id_keyword_dict = {}\n",
    "        word_list = list(set(word_list) & set(keyword_list))\n",
    "        id_keyword_dict[i] = word_list\n",
    "    return id_keyword_dict\n",
    "\n",
    "keyword_path = '../keyword/*.csv'\n",
    "keyword_list = get_keyword_list(keyword_path)\n",
    "# keyword_list = pd.read_csv('../keyword/20181114_08_jrw_train_key.csv', header=None)[0].values\n",
    "\n",
    "# 元データの単語からキーワードのみ残す\n",
    "train_keyword_dict = keyword_filter(train_word_dict, keyword_list)\n",
    "print(train_keyword_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 各テキストの単語リストをループで流し、各単語の類語を取得していく\n",
    "#### WordNet DBを使用\n",
    "#### 類語リストを出力したら、再度キーワードリストを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True\n",
    "test_word = 'うるさい'\n",
    "wn_test = True\n",
    "wn_test = False\n",
    "def get_similar_word(id_word_dict, keyword_list, original=False):\n",
    "    sim_dict = {}\n",
    "    no_wn_list = []\n",
    "    for i, word_list in tqdm(id_word_dict.items()):\n",
    "        tmp_list = []\n",
    "        feature_keywords = []\n",
    "    \n",
    "        for word in word_list:\n",
    "            # WordNet DB Test\n",
    "            if wn_test:\n",
    "                similar_words = search_similar_words(test_word, syn_num=3, smw_num=10, display=True)\n",
    "                sys.exit()\n",
    "            similar_words = search_similar_words(word, syn_num=3, smw_num=10, display=False)\n",
    "    \n",
    "            # 類語がない（DBに登録がない）単語も存在する\n",
    "            if similar_words==0:\n",
    "                no_wn_list.append(word)\n",
    "                continue\n",
    "            tmp_list.append(similar_words)\n",
    "    \n",
    "        # nestしてるlistを1次元にする\n",
    "        feature_words = list(chain.from_iterable(tmp_list))\n",
    "        # original=Trueの場合、元の単語リストと類語リストを合わせてデータセットに格納する\n",
    "        if original:\n",
    "            feature_words = list(set(feature_words) & set(keyword_list)) + word_list\n",
    "        # テキスト別に類語リストを辞書へ格納\n",
    "        sim_dict[i] = feature_words\n",
    "        \n",
    "    logger.info(f\"Complete Get Similar Words!!\")\n",
    "    \n",
    "    return sim_dict, list(set(no_wn_list))\n",
    "\n",
    "# 類語のみのリストを出力\n",
    "train_similarword_dict, no_wn_list = get_similar_word(train_keyword_dict, keyword_list)\n",
    "\n",
    "#========================================================================\n",
    "# 取得した類語リストの保存\n",
    "#========================================================================\n",
    "if save:\n",
    "    # 類語リスト\n",
    "    tmp_list = [word_list for word_list in train_similarword_dict.values()]\n",
    "    similar_keywords = list(set(list(chain.from_iterable(tmp_list))))\n",
    "    print(f\"Count Similar Word: {len(similar_keywords)}\")\n",
    "    df_cnt = pd.Series(similar_keywords, name='word').sort_values(ascending=False)\n",
    "    df_cnt.to_csv(f'../output/{start_time[:11]}_similar_word_list.csv', index=False)\n",
    "    # 類語のなかった単語リスト\n",
    "    df_no_similar = pd.Series(no_wn_list, name='word')\n",
    "    df_no_similar.to_csv('../output/{start_time[:11]_jrw_no_similar_word.csv}', index=False)\n",
    "    \n",
    "    display(df_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T09:01:24.256045Z",
     "start_time": "2018-11-15T09:01:24.216081Z"
    }
   },
   "source": [
    "**類語から作成したキーワードリストを追加で読み込む**  \n",
    "**単語リストをキーワードのみに絞る**  \n",
    "**前の実行部分で取得したtrain_word_dictを使用しないとunique_idと対応させるのが手間**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True\n",
    "new_keyword_list = get_keyword_list(keyword_path)\n",
    "\n",
    "new_train_dict = keyword_filter(train_word_dict, new_keyword_list)\n",
    "new_test_dict = keyword_filter(test_word_dict, new_keyword_list)\n",
    "        \n",
    "#========================================================================\n",
    "# 今回の類語リストでは、キーワードのみを残す\n",
    "# 各テキストの単語リストをループで流し、各単語の類語を取得していく\n",
    "# WordNet DB\n",
    "# ========================================================================\n",
    "# keyword\n",
    "new_train_dict, train_no_wn_list = get_similar_word(new_train_dict, new_keyword_list, original=True)\n",
    "new_test_dict, _ = get_similar_word(new_test_dict, new_keyword_list, original=True)\n",
    "\n",
    "#========================================================================\n",
    "# 取得した類語リストの保存\n",
    "#========================================================================\n",
    "if save:\n",
    "    train = pd.Series(new_train_dict, name='word').to_frame().reset_index().rename(columns={'index':'unique_id'})\n",
    "    test = pd.Series(new_test_dict, name='word').to_frame().reset_index().rename(columns={'index':'unique_id'})\n",
    "    \n",
    "    train_path = f'../output/{start_time[:11]}_train_dataset'\n",
    "    test_path = f'../output/{start_time[:11]}_test_dataset'\n",
    "    utils.to_pkl_gzip(obj=train, path=train_path)\n",
    "    utils.to_pkl_gzip(obj=test, path=test_path)\n",
    "    print(\"Train\")\n",
    "    display(train.head(10))\n",
    "    print(\"Test\")\n",
    "    display(test.head(10))\n",
    "    # 類語のなかった単語リスト\n",
    "    df_no_similar = pd.Series(no_wn_list, name='word')\n",
    "    df_no_similar.to_csv('../output/{start_time[:11]_jrw_no_similar_allword.csv}', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 機械学習モデルに入力するため、Bag of Wordsに変換する為のgensimオブジェクトを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from nlp_utils import word_dense\n",
    "\n",
    "# Dataset Load\n",
    "train_dict = utils.read_pkl_gzip(train_path+'.gz').dropna()\n",
    "train_dict = train_dict.set_index('unique_id')['word'].to_dict()\n",
    "# train_dict['word'] = train_dict['word'].map(lambda x: x.replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace(' ', '').split(','))\n",
    "# test_dict = pd.read_csv('../output/20181113_15_test_dataset.csv')\n",
    "test_dict = utils.read_pkl_gzip(test_path+'.gz').dropna()\n",
    "test_dict = test_dict.set_index('unique_id')['word'].to_dict()\n",
    "# test_dict['word'] = test_dict['word'].map(lambda x: x.replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace(' ', '').split(','))\n",
    "dict_path = f'../output/{start_time[:11]}_jrw_gensim_dict_2016.txt'\n",
    "\n",
    "# 全ワードでgensim Dictionaryを作成し、Bag of Wordsを出力できるようにする\n",
    "# 辞書は保存しておく\n",
    "all_words = []\n",
    "all_words.append(list(set(list(chain.from_iterable(list(train_dict.values()))))))\n",
    "logger.info(\"Gensim make dictionary Start!!\")\n",
    "gs_dict = corpora.Dictionary(all_words)\n",
    "gs_dict.save_as_text(dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語リストになっている特徴セットをBag of Wordsでベクトル変換\n",
    "#### 合わせてdate系のfeatureも作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_dict = corpora.Dictionary.load_from_text(dict_path)\n",
    "raw_train = pd.read_excel('../input/jr_train2015.xls')[['unique_id', 'date', 'contents', 'target']].dropna()\n",
    "# raw_train2016 = pd.read_excel('../input/jr_train2016.xls')[['unique_id', 'date', 'contents', 'target']].dropna()\n",
    "# raw_train = pd.concat([raw_trian, raw_train2016], axis=0)\n",
    "raw_test = pd.read_excel('../input/jr_test2017.xls')[['unique_id', 'date', 'contents', 'target']].dropna()\n",
    "\n",
    "\n",
    "def make_featureset(words_dict, gs_dict):\n",
    "    '''\n",
    "    Explain:\n",
    "        dict型で単語リストをもったデータセットをBoWに変換する\n",
    "    Args:\n",
    "        words_dict(dict): key: unique_id, value: テキストを分かち書きした単語リスト\n",
    "    Return:\n",
    "    '''\n",
    "    dense_list = []\n",
    "    for word_list in words_dict.values():\n",
    "        if len(word_list)==0:\n",
    "            word_list.append('')\n",
    "        dense = word_dense(list(word_list), gs_dict)\n",
    "        dense_list.append(dense)\n",
    "\n",
    "    df = pd.DataFrame(dense_list, index=words_dict.keys()).reset_index().rename(columns={'index':key})\n",
    "    df.columns = [gs_dict[col]  if col not in ignore_list else col for col in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_month(df):\n",
    "    df['month'] = df['date'].map(lambda x: int(str(x.strftime(\"%m\"))))\n",
    "    df['max_temperature'] = df['month'].map(lambda x:\n",
    "                                                9 if x==1\n",
    "                                                else 10 if x==2\n",
    "                                                else 13 if x==3\n",
    "                                                else 20 if x==4\n",
    "                                                else 24 if x==5\n",
    "                                                else 27 if x==6\n",
    "                                                else 31 if x==7\n",
    "                                                else 33 if x==8\n",
    "                                                else 29 if x==9\n",
    "                                                else 23 if x==10\n",
    "                                                else 17 if x==11\n",
    "                                                else 12 if x==12\n",
    "                                                else 20\n",
    "                                                )\n",
    "    df['min_temperature'] = df['month'].map(lambda x:\n",
    "                                                2 if x==1\n",
    "                                                else 2 if x==2\n",
    "                                                else 5 if x==3\n",
    "                                                else 10 if x==4\n",
    "                                                else 15 if x==5\n",
    "                                                else 20 if x==6\n",
    "                                                else 24 if x==7\n",
    "                                                else 25 if x==8\n",
    "                                                else 21 if x==9\n",
    "                                                else 15 if x==10\n",
    "                                                else 9 if x==11\n",
    "                                                else 4 if x==12\n",
    "                                                else 20\n",
    "                                                )\n",
    "    df['precipitation_amount'] = df['month'].map(lambda x:\n",
    "                                                     46  if x==1\n",
    "                                                else 60  if x==2\n",
    "                                                else 102 if x==3\n",
    "                                                else 134 if x==4\n",
    "                                                else 139 if x==5\n",
    "                                                else 206 if x==6\n",
    "                                                else 157 if x==7\n",
    "                                                else 95  if x==8\n",
    "                                                else 172 if x==9\n",
    "                                                else 108 if x==10\n",
    "                                                else 65  if x==11\n",
    "                                                else 34  if x==12\n",
    "                                                else 100\n",
    "                                                )\n",
    "    df['cos_month'] = df['month'].map(lambda x: np.cos(x*30))\n",
    "    df.drop(['date', 'month'], axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "class_list = list(raw_train.dropna()['target'].drop_duplicates().values)\n",
    "y_train = raw_train['target'].values\n",
    "y_test = raw_test['target'].values\n",
    "\n",
    "train = make_featureset(train_dict, gs_dict)\n",
    "train['date'] = raw_train['date']\n",
    "train[target] = y_train\n",
    "\n",
    "test = make_featureset(test_dict, gs_dict)\n",
    "test['date'] = raw_test['date']\n",
    "test[target] = y_test\n",
    "train = train.loc[train[target].dropna().index, :].dropna()\n",
    "test= test.loc[test[target].dropna().index, :].dropna()\n",
    "del raw_train, raw_test\n",
    "train = feature_month(train)\n",
    "test = feature_month(test)\n",
    "try:\n",
    "    train.drop('', axis=1, inplace=True)\n",
    "except ValueError:\n",
    "    pass\n",
    "try:\n",
    "    test.drop('', axis=1, inplace=True)\n",
    "except ValueError:\n",
    "    pass\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベクトル化したデータを可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.drop('date', axis=1, inplace=True)\n",
    "# test.drop('date', axis=1, inplace=True)\n",
    "print(train.shape)\n",
    "display(train.head())\n",
    "print(test.shape)\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 機械学習による分類を実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Original Library\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/library/')\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis')\n",
    "from model.lightgbm_ex import lightgbm_ex as lgb_ex\n",
    "from model.params_lgbm import train_params_nlp\n",
    "from preprocessing import factorize_categoricals, get_dummies, ordinal_encode, get_ordinal_mapping\n",
    "\n",
    "#========================================================================\n",
    "# ML Args\n",
    "#========================================================================\n",
    "model_type = 'lgb'\n",
    "params = train_params_nlp()\n",
    "multiclass = True\n",
    "# metric = 'auc'\n",
    "metric = 'accuracy'\n",
    "early_stopping_rounds = 50\n",
    "num_boost_round = 1000\n",
    "learning_rate = 0.1\n",
    "fold=4\n",
    "fold_type='stratified'\n",
    "group_col_name=''\n",
    "dummie=1\n",
    "oof_flg=True\n",
    "#  oof_flg=False\n",
    "LGBM = lgb_ex(logger=logger, metric=metric, model_type=model_type, ignore_list=ignore_list)\n",
    "\n",
    "#========================================================================\n",
    "# Train & Prediction Start\n",
    "#========================================================================\n",
    "class_list = ['multiclass']\n",
    "\n",
    "train_class = train[target].values\n",
    "test_class = test[target].values\n",
    "label_method = lambda x:1 if x==text_class else 0\n",
    "\n",
    "#========================================================================\n",
    "# 元コードから無理やり引っ張ったので無駄な処理多し\n",
    "#========================================================================\n",
    "for i, text_class in enumerate(class_list):\n",
    "    params['objective'] = 'multiclass'\n",
    "    params['num_class'] = 13\n",
    "    del params['metric']\n",
    "    LGBM.metric = 'accuracy'\n",
    "    \n",
    "    # クラスのラベル化\n",
    "    ignore_list.remove(target)\n",
    "    train[target] = train_class\n",
    "    train, _, _ = LGBM.data_check(train=train, test=[], target=target, encode='ordinal')\n",
    "    ignore_list.append(target)\n",
    "    category_map = train[target].to_frame()\n",
    "    category_map['text_class'] = train_class\n",
    "    category_map = category_map.drop_duplicates().set_index(target).to_dict()['text_class']\n",
    "    train[target] += -1 # ordinal encodeでclass設定してるので、1始まりになってる\n",
    "\n",
    "    logger.info(f'''\n",
    "#========================================================================\n",
    "# Text Class: {text_class} Classifier Start!!\n",
    "#========================================================================''')\n",
    "\n",
    "    if len(test)==0:\n",
    "        LGBM = LGBM.cross_validation(\n",
    "            train=train\n",
    "            ,key=''\n",
    "            ,target=target\n",
    "            ,fold_type=fold_type\n",
    "            ,fold=fold\n",
    "            ,group_col_name=group_col_name\n",
    "            ,params=params\n",
    "            ,num_boost_round=num_boost_round\n",
    "            ,early_stopping_rounds=early_stopping_rounds\n",
    "        )\n",
    "    else:\n",
    "        test[target] = test_class\n",
    "        test = LGBM.decoder.fit_transform(test)\n",
    "        test[target] += -1\n",
    "\n",
    "        LGBM = LGBM.cross_prediction(\n",
    "            train=train\n",
    "            ,test=test\n",
    "            ,key=key\n",
    "            ,target=target\n",
    "            ,fold_type=fold_type\n",
    "            ,fold=fold\n",
    "            ,group_col_name=group_col_name\n",
    "            ,params=params\n",
    "            ,num_boost_round=num_boost_round\n",
    "            ,early_stopping_rounds=early_stopping_rounds\n",
    "            ,oof_flg=oof_flg\n",
    "        )\n",
    "\n",
    "    LGBM.cv_feim.to_csv(f'../valid/{start_time[4:12]}_{model_type}_jrw_{text_class}_feat{len(LGBM.use_cols)}_CV{LGBM.cv_score}_lr{learning_rate}.csv', index=False)\n",
    "\n",
    "    if params['objective']=='multiclass':\n",
    "        break\n",
    "\n",
    "#========================================================================\n",
    "# Trainデータの目的変数と予測値を比較するDF\n",
    "# Column: unique_id | target | prediction\n",
    "#========================================================================\n",
    "train_stack = LGBM.train_stack\n",
    "train_stack[target] = train_stack[target].map(lambda x: category_map[x+1])\n",
    "train_stack['prediction'] = train_stack['prediction'].map(lambda x: category_map[x+1])\n",
    "train_stack.to_csv(f'../output/{start_time[:11]}_jrw_train_stack_CV{LGBM.cv_score}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import japanize_matplotlib\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(12, 16))\n",
    "sns.barplot(data=LGBM.cv_feim.sort_values(by='avg_importance', ascending=False).iloc[:50, :], x='avg_importance', y='feature')\n",
    "plt.show()\n",
    "# viz.set_xticklabels(df_cnt['index'], rotation=90)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
