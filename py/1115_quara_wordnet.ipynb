{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No such file or directory: '/home/ubuntu/nltk_data/corpora/wordnet_ic/ic-bro.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-53ccacb53ba9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwordnet_ic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mbrown_ic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet_ic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ic-bro.dat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0msemcor_ic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet_ic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ic-semcor.dat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mic\u001b[0;34m(self, icfile)\u001b[0m\n\u001b[1;32m   1925\u001b[0m         \u001b[0mic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1926\u001b[0m         \u001b[0mic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mVERB\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1927\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0micfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# skip the header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1929\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/api.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \"\"\"\n\u001b[1;32m    212\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, fileid)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, _path)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No such file or directory: %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: No such file or directory: '/home/ubuntu/nltk_data/corpora/wordnet_ic/ic-bro.dat'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "HOME = os.path.expanduser('~')\n",
    "sys.path.append(f\"{HOME}/kaggle/data_analysis/library/\")\n",
    "import utils\n",
    "from pararell_utils import pararell_process\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk import word_tokenize\n",
    "brown_ic = wordnet_ic.ic('ic-bro.dat')\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_wordnet_similarity_path_wup(w1, w2):\n",
    "    w1_sets = wn.synsets(w1)\n",
    "    w2_sets = wn.synsets(w2)\n",
    "    path_val = []\n",
    "    wup_val = []\n",
    "    for w1, w2 in zip(w1_sets, w2_sets):\n",
    "        path_val.append(w1.path_similarity(w2))\n",
    "        wup_val.append(w1.wup_similarity(w2))\n",
    "    return np.max(path_val), np.max(wup_val)\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Get Synonym List\n",
    "#========================================================================\n",
    "def get_wordnet_lemma(word):\n",
    "    lemma_list = []\n",
    "    for w in wn.synsets(word):\n",
    "        lemma_list+=w.lemma_names()\n",
    "    return list(set(lemma_list) - set([word]))\n",
    "\n",
    "key = 'qid'\n",
    "qt = 'question_text'\n",
    "train = utils.read_df_pkl('../input/train*.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 4.29 µs\n",
      "50000/1306122\n",
      "100000/1306122\n",
      "150000/1306122\n",
      "200000/1306122\n",
      "250000/1306122\n",
      "300000/1306122\n",
      "350000/1306122\n",
      "400000/1306122\n",
      "450000/1306122\n",
      "500000/1306122\n",
      "550000/1306122\n",
      "600000/1306122\n",
      "650000/1306122\n",
      "700000/1306122\n",
      "750000/1306122\n",
      "800000/1306122\n",
      "850000/1306122\n",
      "900000/1306122\n",
      "950000/1306122\n",
      "1000000/1306122\n",
      "1050000/1306122\n",
      "1100000/1306122\n",
      "1150000/1306122\n",
      "1200000/1306122\n",
      "1250000/1306122\n",
      "1300000/1306122\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "id_list = train[key]\n",
    "qt_list = train[qt]\n",
    "\n",
    "lemma_dict = {}\n",
    "for uid, doc in zip(id_list, qt_list):\n",
    "    word_list = doc.split()\n",
    "    lemma_list = []\n",
    "    for word in word_list:\n",
    "        lemma_list += get_wordnet_lemma(word)\n",
    "    lemma_dict[uid] = lemma_list\n",
    "    if len(lemma_dict)%50000==0:\n",
    "        print(f'{len(lemma_dict)}/{len(id_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coif', 'manage', 'coiffure', 'set', 'make_out', 'cause', 'execute', 'behave', 'exercise', 'come', 'practice', 'fare', 'arrange', 'do', 'get_along', 'practise', 'suffice', 'perform', 'dress', 'serve', 'act', 'answer', 'make', 'coiffe', 'Quebec_City', 'nationalist', 'patriot', 'escort', 'get_wind', 'realise', 'understand', 'view', 'ascertain', 'examine', 'learn', 'image', 'experience', 'reckon', 'date', 'picture', 'go_through', 'see_to_it', 'ensure', 'get_word', 'run_into', 'get_a_line', 'look', 'regard', 'find_out', 'interpret', 'pick_up', 'determine', 'visualise', 'hear', 'envision', 'discover', 'go_steady', 'insure', 'realize', 'check', 'meet', 'construe', 'take_care', 'take_in', 'come_across', 'run_across', 'visit', 'find', 'control', 'witness', 'project', 'fancy', 'figure', 'assure', 'go_out', 'watch', 'catch', 'encounter', 'consider', 'attend', 'visualize', 'state', 'responsibility', 'A', 'AS', 'equally', 'atomic_number_33', 'every_bit', 'adenine', 'arsenic', 'Eastern_Samoa', 'deoxyadenosine_monophosphate', 'As', 'group_A', 'ampere', 'antiophthalmic_factor', 'American_Samoa', 'angstrom', 'axerophthol', 'angstrom_unit', 'a', 'amp', 'type_A', 'vitamin_A', 'A', 'angstrom', 'axerophthol', 'angstrom_unit', 'adenine', 'amp', 'deoxyadenosine_monophosphate', 'group_A', 'ampere', 'antiophthalmic_factor', 'vitamin_A', 'type_A', 'Carry_Nation', 'Nation', 'land', 'country', 'commonwealth', 'state', 'res_publica', 'Carry_Amelia_Moore_Nation', 'body_politic', 'indium', 'Indiana', 'In', 'Hoosier_State', 'IN', 'inward', 'inch', 'atomic_number_49', 'inwards']\n",
      "['coif', 'manage', 'coiffure', 'bash', 'make_out', 'doh', 'execute', 'cause', 'behave', 'exercise', 'set', 'practice', 'come', 'ut', 'fare', 'arrange', 'do', 'get_along', 'DO', 'brawl', 'practise', 'suffice', 'perform', 'dress', 'Doctor_of_Osteopathy', 'serve', 'act', 'answer', 'make', 'coiffe', 'get', 'give', 'wealthy_person', 'suffer', 'rich_person', 'take', 'take_in', 'accept', 'cause', 'own', 'birth', 'throw', 'consume', 'bear', 'sustain', 'feature', 'experience', 'give_birth', 'receive', 'induce', 'stimulate', 'possess', 'hold', 'deliver', 'let', 'ingest', 'have_got', 'make', 'AN', 'Associate_in_Nursing', 'assume', 'espouse', 'follow', 'dramatize', 'take', 'dramatise', 'take_in', 'adopt', 'acquire', 'embrace', 'take_over', 'take_up', 'sweep_up', 'borrow', 'take_on', 'adoptive', 'promote', 'boost', 'advance', 'further', 'masses', 'the_great_unwashed', 'citizenry', 'hoi_polloi', 'multitude', 'mass', 'assume', 'espouse', 'follow', 'acquire', 'dramatize', 'embrace', 'take_over', 'take_up', 'take', 'dramatise', 'sweep_up', 'borrow', 'take_in', 'take_on', 'non']\n",
      "['why', 'wherefore', 'Department_of_Energy', 'Energy', 'coif', 'manage', 'coiffure', 'set', 'make_out', 'cause', 'execute', 'behave', 'exercise', 'doe', 'come', 'practice', 'fare', 'arrange', 'do', 'get_along', 'practise', 'suffice', 'perform', 'Energy_Department', 'DOE', 'dress', 'serve', 'act', 'answer', 'make', 'coiffe', 'speed', 'touch', 'touch_on', 'bear_upon', 'impress', 'dissemble', 'move', 'sham', 'pretend', 'impact', 'bear_on', 'regard', 'strike', 'involve', 'feign', 'Department_of_Energy', 'Energy', 'coif', 'manage', 'coiffure', 'set', 'make_out', 'cause', 'execute', 'behave', 'exercise', 'doe', 'come', 'practice', 'fare', 'arrange', 'do', 'get_along', 'practise', 'suffice', 'perform', 'Energy_Department', 'DOE', 'dress', 'serve', 'act', 'answer', 'make', 'coiffe', 'speed', 'touch', 'touch_on', 'bear_upon', 'impress', 'dissemble', 'move', 'sham', 'pretend', 'impact', 'bear_on', 'regard', 'strike', 'involve', 'feign', 'distance', 'infinite', 'blank_space', 'blank', 'quad', 'outer_space', 'place']\n"
     ]
    }
   ],
   "source": [
    "for key in list(lemma_dict.keys())[:3]:\n",
    "    print(lemma_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.to_pkl_gzip(obj=lemma_dict, path='../input/1116_train_wordnet_lemma_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
