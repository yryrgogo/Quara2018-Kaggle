{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T09:53:43.606080Z",
     "start_time": "2018-11-10T09:53:43.430947Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-17 11:06:45,902 utils 353 [INFO]    [logger_func] start \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import sys\n",
    "import re\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import tensorflow as tf\n",
    "print(tf.test.is_built_with_cuda())\n",
    "\n",
    "pd.set_option('max_columns', 200)\n",
    "pd.set_option('max_rows', 200)\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    \"\"\"\n",
    "    Taken from Konstantin Lopuhin https://www.kaggle.com/lopuhin\n",
    "    in script named : Mercari Golf: 0.3875 CV in 75 LOC, 1900 s\n",
    "    https://www.kaggle.com/lopuhin/mercari-golf-0-3875-cv-in-75-loc-1900-s\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "    \n",
    "# Pararell\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "def pararell_process(func, arg_list, cpu_cnt=multiprocessing.cpu_count()):\n",
    "    process = Pool(cpu_cnt)\n",
    "    callback = process.map(func, arg_list)\n",
    "    process.close()\n",
    "    process.terminate()\n",
    "    return callback\n",
    "    \n",
    "# Global Variables\n",
    "key = 'qid'\n",
    "qt = 'question_text'\n",
    "seed = 1208\n",
    "\n",
    "# Load Data\n",
    "with timer(\"Load Data\"):\n",
    "    pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T10:01:58.493129Z",
     "start_time": "2018-11-10T10:01:58.480016Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 12s, sys: 1.12 s, total: 2min 13s\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "#========================================================================\n",
    "# Make Train Validation\n",
    "# Tokenizer\n",
    "#========================================================================\n",
    "\n",
    "## some config values \n",
    "embed_size = 300 # how big is each word vector\n",
    "# Current Best 30000\n",
    "max_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a question to use\n",
    "\n",
    "with timer(\"Make Train Vaidation Set & Tokenizer\"):\n",
    "    \n",
    "    ## split to train and val\n",
    "    train_df, val_df = train_test_split(tmp_train, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    ## fill up the missing values\n",
    "    train_X = train_df[\"question_text\"].fillna(\"_na_\").values\n",
    "    val_X = val_df[\"question_text\"].fillna(\"_na_\").values\n",
    "    # test_X = test_df[\"question_text\"].fillna(\"_na_\").values\n",
    "    \n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    val_X = tokenizer.texts_to_sequences(val_X)\n",
    "    # test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    \n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    val_X = pad_sequences(val_X, maxlen=maxlen)\n",
    "    # test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "    \n",
    "    ## Get the target values\n",
    "    train_y = train_df['target'].values\n",
    "    val_y = val_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T10:08:08.365358Z",
     "start_time": "2018-11-10T10:08:08.360417Z"
    }
   },
   "outputs": [],
   "source": [
    "#========================================================================\n",
    "# No PreTrain Model\n",
    "#========================================================================\n",
    "def no_pretrain_NN():\n",
    "    with timer(\"Create No PreTrain Model\"):\n",
    "        inp = Input(shape=(maxlen,))\n",
    "        x = Embedding(max_features, embed_size)(inp)\n",
    "        # x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "        x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "        x = GlobalMaxPool1D()(x)\n",
    "        x = Dense(16, activation=\"relu\")(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(1, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs=inp, outputs=x)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        print(model.summary())\n",
    "        \n",
    "    with timer(\"Model Fitting\"):\n",
    "        ## Train the model \n",
    "        model.fit(train_X, train_y, batch_size=512, epochs=2,\n",
    "                  validation_data=(val_X, val_y)\n",
    "                 )\n",
    "        \n",
    "    with timer(\"Prediction & Get F1 score\"):\n",
    "        pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n",
    "        for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "            thresh = np.round(thresh, 2)\n",
    "            print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_noemb_val_y>thresh).astype(int))))\n",
    "        del model, inp, x\n",
    "        import gc; gc.collect()\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T10:21:50.258378Z",
     "start_time": "2018-11-10T10:21:50.254689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1044897 samples, validate on 261225 samples\n",
      "Epoch 1/2\n",
      "1044897/1044897 [==============================] - 53s 51us/step - loss: 0.1665 - acc: 0.9429 - val_loss: 0.1493 - val_acc: 0.9456\n",
      "Epoch 2/2\n",
      "1044897/1044897 [==============================] - 51s 49us/step - loss: 0.1423 - acc: 0.9478 - val_loss: 0.1488 - val_acc: 0.9456\n"
     ]
    }
   ],
   "source": [
    "#========================================================================\n",
    "# Cross Validation\n",
    "#========================================================================\n",
    "with timer(\"Make Train Vaidation Set & Tokenizer\"):\n",
    "    \n",
    "    ## fill up the missing values\n",
    "    train_X = tmp_train[\"question_text\"].fillna(\"_na_\").values\n",
    "    \n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    # test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    \n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    # test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "    \n",
    "    ## Get the target values\n",
    "    train_y = train_df['target'].values\n",
    "\n",
    "    # KFold\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    if fold_type == 'stratified':\n",
    "        folds = StratifiedKFold(n_splits=fold, shuffle=True, random_state=seed)  # 1\n",
    "        kfold = folds.split(train_X, train_y)\n",
    "\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kfold):\n",
    "        x_train, x_val = train_X[train_idx], train_X[val_idx]\n",
    "        y_train, y_val = train_y[train_idx], train_y[val_idx] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
