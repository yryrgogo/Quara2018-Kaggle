{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is the purpose of this experiment?\n",
    "WordNetで各テキストのlemmaを取得し, そのlemmaのみで学習モデルを作ることで, 多様性を持ったモデルができるか確認する\n",
    "### 2. Why do you this?\n",
    "WordNetで取得できる類語は, オリジナルテキストと違った特徴を持ち, モデルの多様性を作るのに有効なのではと思った.\n",
    "### 3. Where are the points of technology and techniques?\n",
    "nltk WordNet\n",
    "### 4. How do you validate the effectiveness?\n",
    "CV & LB  \n",
    "lemmaのみのデータセットで精度はでない為、これによる各validationの予測値をstackして特徴量を作る。それをLightGBMとNNに追加し、CVを見る。まずはtrain test splitでOK.\n",
    "### 5. What will you do next?\n",
    "spacy entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T09:53:43.606080Z",
     "start_time": "2018-11-10T09:53:43.430947Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import sys\n",
    "import re\n",
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import tensorflow as tf\n",
    "print(tf.test.is_built_with_cuda())\n",
    "\n",
    "pd.set_option('max_columns', 200)\n",
    "pd.set_option('max_rows', 200)\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    \"\"\"\n",
    "    Taken from Konstantin Lopuhin https://www.kaggle.com/lopuhin\n",
    "    in script named : Mercari Golf: 0.3875 CV in 75 LOC, 1900 s\n",
    "    https://www.kaggle.com/lopuhin/mercari-golf-0-3875-cv-in-75-loc-1900-s\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "    \n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "def pararell_process(func, arg_list, cpu_cnt=multiprocessing.cpu_count()):\n",
    "    process = Pool(cpu_cnt)\n",
    "    callback = process.map(func, arg_list).get(600)\n",
    "    process.close()\n",
    "    process.terminate()\n",
    "    return callback\n",
    "    \n",
    "\n",
    "key = 'qid'\n",
    "qt = 'question_text'\n",
    "seed = 1208\n",
    "\n",
    "with timer(\"Load Data\"):\n",
    "    pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-17 22:24:52,443 utils 353 [INFO]    [logger_func] start \n",
      "100%|██████████| 3/3 [00:01<00:00,  2.45it/s]\n"
     ]
    }
   ],
   "source": [
    "HOME = os.path.expanduser('~')\n",
    "sys.path.append(f\"{HOME}/kaggle/data_analysis/library/\")\n",
    "import utils\n",
    "from utils import logger_func, get_categorical_features, get_numeric_features, pararell_process\n",
    "logger = logger_func()\n",
    "train = utils.read_df_pkl('../input/train*.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Make lemma DF] done in 5 s\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def get_wordnet_similarity_path_wup(w1, w2):\n",
    "    w1_sets = wn.synsets(w1)\n",
    "    w2_sets = wn.synsets(w2)\n",
    "    path_val = []\n",
    "    wup_val = []\n",
    "    for w1, w2 in zip(w1_sets, w2_sets):\n",
    "        path_val.append(w1.path_similarity(w2))\n",
    "        wup_val.append(w1.wup_similarity(w2))\n",
    "    return np.max(path_val), np.max(wup_val)\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Get lemma List\n",
    "#========================================================================\n",
    "def get_wordnet_lemma(word):\n",
    "    lemma_list = []\n",
    "    for w in wn.synsets(word):\n",
    "        lemma_list+=w.lemma_names()\n",
    "    return list(set(lemma_list) - set([word]))\n",
    "\n",
    "\n",
    "# まずは全てのlemmaを取得する\n",
    "id_list = train[key]\n",
    "qt_list = train[qt]\n",
    "def pararell_get_lemma(args):\n",
    "    uid = args[0]\n",
    "    doc = args[1]\n",
    "    word_list = doc.split()\n",
    "    tmp_dict = {}\n",
    "    lemma_list = []\n",
    "    for word in word_list:\n",
    "        lemma_list += get_wordnet_lemma(word)\n",
    "    if original:\n",
    "        tmp_dict[uid] = word_list + lemma_list\n",
    "    else:\n",
    "        tmp_dict[uid] = lemma_list\n",
    "    return tmp_dict\n",
    "        \n",
    "with timer(\"Get lemma from Wordnet\"):\n",
    "    p_list = pararell_process(pararell_get_lemma, zip(id_list, qt_list))\n",
    "with timer(\"Make lemma dataset\"):\n",
    "    lemma_dict = {}\n",
    "    [lemma_dict.update(p) for p in p_list]\n",
    "\n",
    "# 取得したlemmmaの一部を残したデータセットにする\n",
    "# これをいくつかのseedでSamplingし，多様な予測値を作ってstackingする\n",
    "def pararell_val_join(args):\n",
    "    uid = args[0]\n",
    "    value = args[1]\n",
    "    df_dict = {}\n",
    "    np.random.seed(seed)\n",
    "    val_len = int(len(value)*0.5)\n",
    "    try:\n",
    "        value = np.random.choice(a=value, size=val_len)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    df_dict[uid] = \" \".join(value)\n",
    "    return df_dict\n",
    "\n",
    "\n",
    "orginal=False\n",
    "\n",
    "# lemma Sampling\n",
    "with timer(\"lemma sampling\"):\n",
    "    p_list = pararell_process(pararell_val_join, lemma_dict.items())\n",
    "with timer(\"Make lemma dataset\"):\n",
    "    tmp_dict = {}\n",
    "    [tmp_dict.update(p) for p in p_list]\n",
    "\n",
    "# lemma dictをDataFrameにする  \n",
    "with timer(\"Make lemma DF\"):\n",
    "    to_df_dict = tmp_dict\n",
    "    # Sampling済のデータセットをDFにする \n",
    "    tmp_train = pd.Series(to_df_dict).to_frame()\n",
    "    tmp_train = tmp_train.join(train.set_index('qid'))\n",
    "    tmp_train.rename(columns={0:qt}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T10:01:58.493129Z",
     "start_time": "2018-11-10T10:01:58.480016Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 12s, sys: 1.12 s, total: 2min 13s\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "#========================================================================\n",
    "# Make Train Validation\n",
    "# Tokenizer\n",
    "#========================================================================\n",
    "\n",
    "## some config values \n",
    "embed_size = 300 # how big is each word vector\n",
    "# Current Best 30000\n",
    "max_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a question to use\n",
    "\n",
    "with timer(\"Make Train Vaidation Set & Tokenizer\"):\n",
    "    \n",
    "    ## split to train and val\n",
    "    train_df, val_df = train_test_split(tmp_train, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    ## fill up the missing values\n",
    "    train_X = train_df[\"question_text\"].fillna(\"_na_\").values\n",
    "    val_X = val_df[\"question_text\"].fillna(\"_na_\").values\n",
    "    # test_X = test_df[\"question_text\"].fillna(\"_na_\").values\n",
    "    \n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    val_X = tokenizer.texts_to_sequences(val_X)\n",
    "    # test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    \n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    val_X = pad_sequences(val_X, maxlen=maxlen)\n",
    "    # test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "    \n",
    "    ## Get the target values\n",
    "    train_y = train_df['target'].values\n",
    "    val_y = val_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T10:08:08.365358Z",
     "start_time": "2018-11-10T10:08:08.360417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 100, 300)          6000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 100, 128)          187392    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 6,189,473\n",
      "Trainable params: 6,189,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#========================================================================\n",
    "# No PreTrain Model\n",
    "#========================================================================\n",
    "def no_pretrain_NN():\n",
    "    with timer(\"Create No PreTrain Model\"):\n",
    "        inp = Input(shape=(maxlen,))\n",
    "        x = Embedding(max_features, embed_size)(inp)\n",
    "        # x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "        x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "        x = GlobalMaxPool1D()(x)\n",
    "        x = Dense(16, activation=\"relu\")(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(1, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs=inp, outputs=x)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        print(model.summary())\n",
    "        \n",
    "    with timer(\"Model Fitting\"):\n",
    "        ## Train the model \n",
    "        model.fit(train_X, train_y, batch_size=512, epochs=2,\n",
    "                  validation_data=(val_X, val_y)\n",
    "                 )\n",
    "        \n",
    "    with timer(\"Prediction & Get F1 score\"):\n",
    "        pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n",
    "        for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "            thresh = np.round(thresh, 2)\n",
    "            print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_noemb_val_y>thresh).astype(int))))\n",
    "        del model, inp, x\n",
    "        import gc; gc.collect()\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T11:47:05.117033Z",
     "start_time": "2018-11-10T11:47:05.112139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 300)          9000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 128)          140544    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 9,142,625\n",
      "Trainable params: 9,142,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#========================================================================\n",
    "# Glove PreTrain Model\n",
    "#========================================================================\n",
    "def glove_pretrain_NN():\n",
    "    with timer(\"Get Glove PreTrain Grad\"):\n",
    "        EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "        def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "        \n",
    "        all_embs = np.stack(embeddings_index.values())\n",
    "        emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "        embed_size = all_embs.shape[1]\n",
    "        \n",
    "        word_index = tokenizer.word_index\n",
    "        nb_words = min(max_features, len(word_index))\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "        for word, i in word_index.items():\n",
    "            if i >= max_features: continue\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    with timer(\"Create Glove PreTrain Model\"):\n",
    "        inp = Input(shape=(maxlen,))\n",
    "        x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "        x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "        x = GlobalMaxPool1D()(x)\n",
    "        x = Dense(16, activation=\"relu\")(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(1, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs=inp, outputs=x)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        print(model.summary())\n",
    "        \n",
    "    with timer(\"Model Fitting\"):\n",
    "        model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))\n",
    "        \n",
    "    with timer(\"Prediction & Get F1 score\"):\n",
    "        pred_glove_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n",
    "        for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "            thresh = np.round(thresh, 2)\n",
    "            print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_glove_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"Make Train Vaidation Set & Tokenizer\"):\n",
    "    \n",
    "    ## fill up the missing values\n",
    "    train_X = tmp_train[\"question_text\"].fillna(\"_na_\").values\n",
    "    \n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    # test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    \n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    # test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "    \n",
    "    ## Get the target values\n",
    "    train_y = train_df['target'].values\n",
    "\n",
    "    # KFold\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    if fold_type == 'stratified':\n",
    "        folds = StratifiedKFold(n_splits=fold, shuffle=True, random_state=seed)  # 1\n",
    "        kfold = folds.split(train_X, train_y)\n",
    "\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kfold):\n",
    "        x_train, x_val = train_X[train_idx], train_X[val_idx]\n",
    "        y_train, y_val = train_y[train_idx], train_y[val_idx] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
